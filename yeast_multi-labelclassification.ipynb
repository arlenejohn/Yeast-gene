{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c3qDlMPAboNx"
   },
   "source": [
    "# Multi-label Classification\n",
    "for COMP47590: Advanced Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jEV0-8sTboN2"
   },
   "source": [
    "Author(s): Arlene John and Vitoria Fahed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMZO654bboN6"
   },
   "source": [
    "## Import Packages Etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1746,
     "status": "error",
     "timestamp": 1582295090950,
     "user": {
      "displayName": "Arlene John",
      "photoUrl": "",
      "userId": "13865756525876402018"
     },
     "user_tz": 0
    },
    "id": "YMk7QlneboN-",
    "outputId": "372f2e75-c003-4d5a-dd98-7cd35ea152d7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "\n",
    "from sklearn.metrics import hamming_loss,make_scorer, accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import neighbors\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.spatial import distance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWBFbGQlyE6a"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EabmUU5wboOH"
   },
   "source": [
    "## Task 0: Load the Yeast Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1830,
     "status": "ok",
     "timestamp": 1582045724652,
     "user": {
      "displayName": "Vitoria Fahed",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCjQgWlu-JChElNuf5JGwyGBtWtmdVOYOx5q6MT=s64",
      "userId": "01295521859694349438"
     },
     "user_tz": 0
    },
    "id": "T8YnIFatboOK",
    "outputId": "721d5c47-8371-4f35-d934-daacaf116514"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Att1</th>\n",
       "      <th>Att2</th>\n",
       "      <th>Att3</th>\n",
       "      <th>Att4</th>\n",
       "      <th>Att5</th>\n",
       "      <th>Att6</th>\n",
       "      <th>Att7</th>\n",
       "      <th>Att8</th>\n",
       "      <th>Att9</th>\n",
       "      <th>Att10</th>\n",
       "      <th>...</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "      <th>Class11</th>\n",
       "      <th>Class12</th>\n",
       "      <th>Class13</th>\n",
       "      <th>Class14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004168</td>\n",
       "      <td>-0.170975</td>\n",
       "      <td>-0.156748</td>\n",
       "      <td>-0.142151</td>\n",
       "      <td>0.058781</td>\n",
       "      <td>0.026851</td>\n",
       "      <td>0.197719</td>\n",
       "      <td>0.041850</td>\n",
       "      <td>0.066938</td>\n",
       "      <td>-0.056617</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.103956</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>-0.098986</td>\n",
       "      <td>-0.054501</td>\n",
       "      <td>-0.007970</td>\n",
       "      <td>0.049113</td>\n",
       "      <td>-0.030580</td>\n",
       "      <td>-0.077933</td>\n",
       "      <td>-0.080529</td>\n",
       "      <td>-0.016267</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.509949</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.293799</td>\n",
       "      <td>0.087714</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>-0.006411</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>-0.040666</td>\n",
       "      <td>-0.024447</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119092</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>-0.002262</td>\n",
       "      <td>0.072254</td>\n",
       "      <td>0.044512</td>\n",
       "      <td>-0.051467</td>\n",
       "      <td>0.074686</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>0.079438</td>\n",
       "      <td>0.062184</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042037</td>\n",
       "      <td>0.007054</td>\n",
       "      <td>-0.069483</td>\n",
       "      <td>0.081015</td>\n",
       "      <td>-0.048207</td>\n",
       "      <td>0.089446</td>\n",
       "      <td>-0.004947</td>\n",
       "      <td>0.064456</td>\n",
       "      <td>-0.133387</td>\n",
       "      <td>0.068878</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Att1      Att2      Att3      Att4      Att5      Att6      Att7  \\\n",
       "0  0.004168 -0.170975 -0.156748 -0.142151  0.058781  0.026851  0.197719   \n",
       "1 -0.103956  0.011879 -0.098986 -0.054501 -0.007970  0.049113 -0.030580   \n",
       "2  0.509949  0.401709  0.293799  0.087714  0.011686 -0.006411 -0.006255   \n",
       "3  0.119092  0.004412 -0.002262  0.072254  0.044512 -0.051467  0.074686   \n",
       "4  0.042037  0.007054 -0.069483  0.081015 -0.048207  0.089446 -0.004947   \n",
       "\n",
       "       Att8      Att9     Att10  ...  Class5  Class6  Class7  Class8  Class9  \\\n",
       "0  0.041850  0.066938 -0.056617  ...       0       0       1       1       0   \n",
       "1 -0.077933 -0.080529 -0.016267  ...       0       0       0       0       0   \n",
       "2  0.013646 -0.040666 -0.024447  ...       0       0       0       0       0   \n",
       "3 -0.007670  0.079438  0.062184  ...       0       0       0       0       0   \n",
       "4  0.064456 -0.133387  0.068878  ...       1       1       0       0       0   \n",
       "\n",
       "   Class10  Class11  Class12  Class13  Class14  \n",
       "0        0        0        1        1        0  \n",
       "1        0        0        0        0        0  \n",
       "2        0        0        1        1        0  \n",
       "3        0        0        0        0        0  \n",
       "4        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('yeast.csv')\n",
    "dataset = dataset.dropna()\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2351,
     "status": "ok",
     "timestamp": 1582045725184,
     "user": {
      "displayName": "Vitoria Fahed",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCjQgWlu-JChElNuf5JGwyGBtWtmdVOYOx5q6MT=s64",
      "userId": "01295521859694349438"
     },
     "user_tz": 0
    },
    "id": "Pi3HgLSjE5nJ",
    "outputId": "e3fc1e3f-c421-413b-e6ec-7028eed2ed5f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Att1</th>\n",
       "      <th>Att2</th>\n",
       "      <th>Att3</th>\n",
       "      <th>Att4</th>\n",
       "      <th>Att5</th>\n",
       "      <th>Att6</th>\n",
       "      <th>Att7</th>\n",
       "      <th>Att8</th>\n",
       "      <th>Att9</th>\n",
       "      <th>Att10</th>\n",
       "      <th>...</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "      <th>Class11</th>\n",
       "      <th>Class12</th>\n",
       "      <th>Class13</th>\n",
       "      <th>Class14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001173</td>\n",
       "      <td>-0.000436</td>\n",
       "      <td>-0.000257</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298717</td>\n",
       "      <td>0.247000</td>\n",
       "      <td>0.177079</td>\n",
       "      <td>0.198593</td>\n",
       "      <td>0.073645</td>\n",
       "      <td>0.104675</td>\n",
       "      <td>0.119570</td>\n",
       "      <td>0.751345</td>\n",
       "      <td>0.744311</td>\n",
       "      <td>0.014067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.097411</td>\n",
       "      <td>0.097885</td>\n",
       "      <td>0.097746</td>\n",
       "      <td>0.096969</td>\n",
       "      <td>0.096909</td>\n",
       "      <td>0.097306</td>\n",
       "      <td>0.097170</td>\n",
       "      <td>0.096803</td>\n",
       "      <td>0.096326</td>\n",
       "      <td>0.096805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457790</td>\n",
       "      <td>0.431356</td>\n",
       "      <td>0.381815</td>\n",
       "      <td>0.399024</td>\n",
       "      <td>0.261246</td>\n",
       "      <td>0.306198</td>\n",
       "      <td>0.324525</td>\n",
       "      <td>0.432323</td>\n",
       "      <td>0.436338</td>\n",
       "      <td>0.117792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.371146</td>\n",
       "      <td>-0.472632</td>\n",
       "      <td>-0.339195</td>\n",
       "      <td>-0.467945</td>\n",
       "      <td>-0.367044</td>\n",
       "      <td>-0.509447</td>\n",
       "      <td>-0.319928</td>\n",
       "      <td>-0.594498</td>\n",
       "      <td>-0.369712</td>\n",
       "      <td>-0.767128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.053655</td>\n",
       "      <td>-0.058734</td>\n",
       "      <td>-0.057526</td>\n",
       "      <td>-0.057149</td>\n",
       "      <td>-0.058461</td>\n",
       "      <td>-0.060212</td>\n",
       "      <td>-0.058445</td>\n",
       "      <td>-0.062849</td>\n",
       "      <td>-0.063472</td>\n",
       "      <td>-0.065010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.003649</td>\n",
       "      <td>-0.003513</td>\n",
       "      <td>0.002892</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>0.005565</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.057299</td>\n",
       "      <td>0.048047</td>\n",
       "      <td>0.061007</td>\n",
       "      <td>0.054522</td>\n",
       "      <td>0.066286</td>\n",
       "      <td>0.059908</td>\n",
       "      <td>0.068892</td>\n",
       "      <td>0.061418</td>\n",
       "      <td>0.064958</td>\n",
       "      <td>0.063096</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.520272</td>\n",
       "      <td>0.614114</td>\n",
       "      <td>0.353241</td>\n",
       "      <td>0.568960</td>\n",
       "      <td>0.307649</td>\n",
       "      <td>0.336971</td>\n",
       "      <td>0.351401</td>\n",
       "      <td>0.454591</td>\n",
       "      <td>0.419852</td>\n",
       "      <td>0.420876</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Att1         Att2         Att3         Att4         Att5  \\\n",
       "count  2417.000000  2417.000000  2417.000000  2417.000000  2417.000000   \n",
       "mean      0.001173    -0.000436    -0.000257     0.000265     0.001228   \n",
       "std       0.097411     0.097885     0.097746     0.096969     0.096909   \n",
       "min      -0.371146    -0.472632    -0.339195    -0.467945    -0.367044   \n",
       "25%      -0.053655    -0.058734    -0.057526    -0.057149    -0.058461   \n",
       "50%       0.003649    -0.003513     0.002892    -0.000153     0.005565   \n",
       "75%       0.057299     0.048047     0.061007     0.054522     0.066286   \n",
       "max       0.520272     0.614114     0.353241     0.568960     0.307649   \n",
       "\n",
       "              Att6         Att7         Att8         Att9        Att10  ...  \\\n",
       "count  2417.000000  2417.000000  2417.000000  2417.000000  2417.000000  ...   \n",
       "mean      0.000475     0.001107     0.000420     0.001076    -0.000009  ...   \n",
       "std       0.097306     0.097170     0.096803     0.096326     0.096805  ...   \n",
       "min      -0.509447    -0.319928    -0.594498    -0.369712    -0.767128  ...   \n",
       "25%      -0.060212    -0.058445    -0.062849    -0.063472    -0.065010  ...   \n",
       "50%       0.000321     0.006179     0.001436     0.003515     0.002432  ...   \n",
       "75%       0.059908     0.068892     0.061418     0.064958     0.063096  ...   \n",
       "max       0.336971     0.351401     0.454591     0.419852     0.420876  ...   \n",
       "\n",
       "            Class5       Class6       Class7       Class8       Class9  \\\n",
       "count  2417.000000  2417.000000  2417.000000  2417.000000  2417.000000   \n",
       "mean      0.298717     0.247000     0.177079     0.198593     0.073645   \n",
       "std       0.457790     0.431356     0.381815     0.399024     0.261246   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "           Class10      Class11      Class12      Class13      Class14  \n",
       "count  2417.000000  2417.000000  2417.000000  2417.000000  2417.000000  \n",
       "mean      0.104675     0.119570     0.751345     0.744311     0.014067  \n",
       "std       0.306198     0.324525     0.432323     0.436338     0.117792  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     1.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     1.000000     1.000000     0.000000  \n",
       "75%       0.000000     0.000000     1.000000     1.000000     0.000000  \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
       "\n",
       "[8 rows x 117 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEgyvtJBEKRB"
   },
   "outputs": [],
   "source": [
    "features=dataset.iloc[:,0:103]\n",
    "labels=dataset.iloc[:,103:117]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Binary Relavance (BR) without Undersampling</th>\n",
       "      <th>Binary Relavance with Undersampling</th>\n",
       "      <th>Optimised BR Model with F1 score (w/o undersampling)</th>\n",
       "      <th>Optimised BR Model with Accuracy (w/o undersampling)</th>\n",
       "      <th>Optimised BR Model with F1 score (undersampled)</th>\n",
       "      <th>Optimised BR Model with accuracy (undersampled)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hamming Loss ValidSet</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score ValidSet</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy ValidSet</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming Loss TestSet</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score TestSet</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy TestSet</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Binary Relavance (BR) without Undersampling  \\\n",
       "Hamming Loss ValidSet                                          0.0   \n",
       "F1 Score ValidSet                                              0.0   \n",
       "Accuracy ValidSet                                              0.0   \n",
       "Hamming Loss TestSet                                           0.0   \n",
       "F1 Score TestSet                                               0.0   \n",
       "Accuracy TestSet                                               0.0   \n",
       "\n",
       "                       Binary Relavance with Undersampling  \\\n",
       "Hamming Loss ValidSet                                  0.0   \n",
       "F1 Score ValidSet                                      0.0   \n",
       "Accuracy ValidSet                                      0.0   \n",
       "Hamming Loss TestSet                                   0.0   \n",
       "F1 Score TestSet                                       0.0   \n",
       "Accuracy TestSet                                       0.0   \n",
       "\n",
       "                       Optimised BR Model with F1 score (w/o undersampling)  \\\n",
       "Hamming Loss ValidSet                                                0.0      \n",
       "F1 Score ValidSet                                                    0.0      \n",
       "Accuracy ValidSet                                                    0.0      \n",
       "Hamming Loss TestSet                                                 0.0      \n",
       "F1 Score TestSet                                                     0.0      \n",
       "Accuracy TestSet                                                     0.0      \n",
       "\n",
       "                       Optimised BR Model with Accuracy (w/o undersampling)  \\\n",
       "Hamming Loss ValidSet                                                0.0      \n",
       "F1 Score ValidSet                                                    0.0      \n",
       "Accuracy ValidSet                                                    0.0      \n",
       "Hamming Loss TestSet                                                 0.0      \n",
       "F1 Score TestSet                                                     0.0      \n",
       "Accuracy TestSet                                                     0.0      \n",
       "\n",
       "                       Optimised BR Model with F1 score (undersampled)  \\\n",
       "Hamming Loss ValidSet                                              0.0   \n",
       "F1 Score ValidSet                                                  0.0   \n",
       "Accuracy ValidSet                                                  0.0   \n",
       "Hamming Loss TestSet                                               0.0   \n",
       "F1 Score TestSet                                                   0.0   \n",
       "Accuracy TestSet                                                   0.0   \n",
       "\n",
       "                       Optimised BR Model with accuracy (undersampled)  \n",
       "Hamming Loss ValidSet                                              0.0  \n",
       "F1 Score ValidSet                                                  0.0  \n",
       "Accuracy ValidSet                                                  0.0  \n",
       "Hamming Loss TestSet                                               0.0  \n",
       "F1 Score TestSet                                                   0.0  \n",
       "Accuracy TestSet                                                   0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to store the values of the evaluation scores to better compare them\n",
    "initialize = np.zeros(shape=6)\n",
    "df = pd.DataFrame({'Binary Relavance (BR) without Undersampling':initialize , 'Binary Relavance with Undersampling':initialize , 'Optimised BR Model with F1 score (w/o undersampling)':initialize , 'Optimised BR Model with Accuracy (w/o undersampling)':initialize, 'Optimised BR Model with F1 score (undersampled)':initialize, 'Optimised BR Model with accuracy (undersampled)':initialize} , index=['Hamming Loss ValidSet','F1 Score ValidSet','Accuracy ValidSet', 'Hamming Loss TestSet', 'F1 Score TestSet', 'Accuracy TestSet'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Binary Relavance without Undersampling</th>\n",
       "      <th>Binary Relavance with Undersampling</th>\n",
       "      <th>Optimal Binary Relavance without Undersampling (F1 score)</th>\n",
       "      <th>Optimal Binary Relavance with Undersampling (F1 score)</th>\n",
       "      <th>Classifier Chain (CC)</th>\n",
       "      <th>Optimal Classifier Chains (F1 score)</th>\n",
       "      <th>Optimal Changed Chain Sequence CC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hamming Loss ValidSet</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score ValidSet</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy ValidSet</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming Loss TestSet</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score TestSet</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy TestSet</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Binary Relavance without Undersampling  \\\n",
       "Hamming Loss ValidSet                                     0.0   \n",
       "F1 Score ValidSet                                         0.0   \n",
       "Accuracy ValidSet                                         0.0   \n",
       "Hamming Loss TestSet                                      0.0   \n",
       "F1 Score TestSet                                          0.0   \n",
       "Accuracy TestSet                                          0.0   \n",
       "\n",
       "                       Binary Relavance with Undersampling  \\\n",
       "Hamming Loss ValidSet                                  0.0   \n",
       "F1 Score ValidSet                                      0.0   \n",
       "Accuracy ValidSet                                      0.0   \n",
       "Hamming Loss TestSet                                   0.0   \n",
       "F1 Score TestSet                                       0.0   \n",
       "Accuracy TestSet                                       0.0   \n",
       "\n",
       "                       Optimal Binary Relavance without Undersampling (F1 score)  \\\n",
       "Hamming Loss ValidSet                                                0.0           \n",
       "F1 Score ValidSet                                                    0.0           \n",
       "Accuracy ValidSet                                                    0.0           \n",
       "Hamming Loss TestSet                                                 0.0           \n",
       "F1 Score TestSet                                                     0.0           \n",
       "Accuracy TestSet                                                     0.0           \n",
       "\n",
       "                       Optimal Binary Relavance with Undersampling (F1 score)  \\\n",
       "Hamming Loss ValidSet                                                0.0        \n",
       "F1 Score ValidSet                                                    0.0        \n",
       "Accuracy ValidSet                                                    0.0        \n",
       "Hamming Loss TestSet                                                 0.0        \n",
       "F1 Score TestSet                                                     0.0        \n",
       "Accuracy TestSet                                                     0.0        \n",
       "\n",
       "                       Classifier Chain (CC)  \\\n",
       "Hamming Loss ValidSet                    0.0   \n",
       "F1 Score ValidSet                        0.0   \n",
       "Accuracy ValidSet                        0.0   \n",
       "Hamming Loss TestSet                     0.0   \n",
       "F1 Score TestSet                         0.0   \n",
       "Accuracy TestSet                         0.0   \n",
       "\n",
       "                       Optimal Classifier Chains (F1 score)  \\\n",
       "Hamming Loss ValidSet                                   0.0   \n",
       "F1 Score ValidSet                                       0.0   \n",
       "Accuracy ValidSet                                       0.0   \n",
       "Hamming Loss TestSet                                    0.0   \n",
       "F1 Score TestSet                                        0.0   \n",
       "Accuracy TestSet                                        0.0   \n",
       "\n",
       "                       Optimal Changed Chain Sequence CC  \n",
       "Hamming Loss ValidSet                                0.0  \n",
       "F1 Score ValidSet                                    0.0  \n",
       "Accuracy ValidSet                                    0.0  \n",
       "Hamming Loss TestSet                                 0.0  \n",
       "F1 Score TestSet                                     0.0  \n",
       "Accuracy TestSet                                     0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to store the values of the evaluation scores to compare Binary Relevance and Classifier Chains\n",
    "initialize = np.zeros(shape=6)\n",
    "df2 = pd.DataFrame({'Binary Relavance without Undersampling':initialize , 'Binary Relavance with Undersampling':initialize,'Optimal Binary Relavance without Undersampling (F1 score)':initialize , 'Optimal Binary Relavance with Undersampling (F1 score)':initialize, 'Classifier Chain (CC)':initialize,'Optimal Classifier Chains (F1 score)':initialize, 'Optimal Changed Chain Sequence CC':initialize} , index=['Hamming Loss ValidSet','F1 Score ValidSet','Accuracy ValidSet', 'Hamming Loss TestSet', 'F1 Score TestSet', 'Accuracy TestSet'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UtYPcABLboOQ"
   },
   "source": [
    "## Task 1: Implement the Binary Relevance Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPq1WYJqS5JX"
   },
   "source": [
    "References: \n",
    "<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "<br>\n",
    "https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GC0CPxjyGhrT"
   },
   "outputs": [],
   "source": [
    "class BinaryRel(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \n",
    "    def __init__(self, model = 'DecisionTree', balanced='unbalanced'):\n",
    "        self.model = model\n",
    "        self.balanced = balanced\n",
    "        \n",
    "    \n",
    "    def undersample(self, X_train, y_train):\n",
    "        # Random Undersampling\n",
    "        X_train_class_0 = X_train[y_train == 0]\n",
    "        X_train_class_1 = X_train[y_train == 1]\n",
    "        count_class_0 = X_train_class_0.iloc[:,0].count() #count number of each class (0 and 1)\n",
    "        count_class_1 = X_train_class_1.iloc[:,0].count()\n",
    "        if count_class_0 > count_class_1: #if 0 class greater than 1 class, randomly sample the 0 class features from the dataframe\n",
    "            x_train_0_under = X_train_class_0.sample(count_class_1)\n",
    "            y_train_0_under = pd.DataFrame(np.zeros((count_class_1, 1)))\n",
    "            x_train_under = pd.concat([x_train_0_under, X_train_class_1], axis=0)\n",
    "            y_train_under = pd.concat([y_train_0_under, pd.DataFrame(np.ones((count_class_1, 1)))], axis=0)\n",
    "        else:\n",
    "            x_train_1_under = X_train_class_1.sample(count_class_0)\n",
    "            y_train_1_under = pd.DataFrame(np.ones((count_class_0, 1)))\n",
    "            x_train_under = pd.concat([X_train_class_0, x_train_1_under], axis=0)\n",
    "            y_train_under = pd.concat([pd.DataFrame(np.zeros((count_class_0, 1))),y_train_1_under], axis=0)\n",
    "        \n",
    "        return x_train_under,y_train_under\n",
    "    \n",
    "    def fit(self, features, labels):\n",
    "        #fit function fits a model per label based on the base classifier specified and keeps all the models in a list that is iteratively appended\n",
    "        \n",
    "        if self.model == 'DecisionTree':\n",
    "            our_model = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "        elif self.model == 'kNN':\n",
    "            our_model = neighbors.KNeighborsClassifier()\n",
    "        elif self.model == 'SVC':\n",
    "            our_model = SVC(kernel='rbf')\n",
    "        elif self.model == 'NaiveBayes':\n",
    "            our_model = GaussianNB()\n",
    "        elif self.model == 'LogisticRegression':\n",
    "            our_model = linear_model.LogisticRegression()\n",
    "        elif self.model == 'Bagging':\n",
    "            our_model = ensemble.BaggingClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 50), n_estimators=10)\n",
    "        elif self.model == 'Boosting':\n",
    "            our_model = ensemble.AdaBoostClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 50), n_estimators=10)\n",
    "        elif self.model == 'RandomForest':\n",
    "            our_model = ensemble.RandomForestClassifier(n_estimators = 200, max_features=15, min_samples_split=100)\n",
    "\n",
    "        our_final_model = list()\n",
    "        \n",
    "        if self.balanced == 'balanced': #check if user requests for balanced data with undersampling\n",
    "            for i in range(len(labels.columns)):\n",
    "                features_under, labels_under = self.undersample(features,labels.iloc[:,i])\n",
    "                our_model = our_model.fit(features_under, labels_under.iloc[:,0])\n",
    "                self.length_ = len(labels.columns)\n",
    "                our_final_model.append(copy.deepcopy(our_model))\n",
    "        else:\n",
    "            for i in range(len(labels.columns)):\n",
    "                our_model = our_model.fit(features, labels.iloc[:,i])\n",
    "                self.length_ = len(labels.columns)\n",
    "                our_final_model.append(copy.deepcopy(our_model))\n",
    "      \n",
    "        self.our_final_model_ = our_final_model\n",
    " \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, features_test):\n",
    "        prediction_proba=np.zeros((len(features_test),2,self.length_))\n",
    "        for k in range(self.length_):\n",
    "            each_model=self.our_final_model_[k]\n",
    "            prediction_proba[:,:,k]=each_model.predict_proba(features_test)\n",
    "            \n",
    "        return prediction_proba\n",
    "            \n",
    "    def predict(self, features_test):\n",
    "        #uses the models found during fit and returns predictions as a matrix\n",
    "        prediction = np.ones((len(features_test), self.length_))\n",
    "        for k in range(self.length_):\n",
    "            each_model = self.our_final_model_[k]\n",
    "            prediction[:,k] = (each_model.predict(features_test)).T\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_fURfOcf2bP"
   },
   "outputs": [],
   "source": [
    "X_train_plus_valid, X_test, y_train_plus_valid, y_test = train_test_split(features, labels, random_state=0, train_size = 0.7)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_plus_valid, y_train_plus_valid, random_state=0, train_size = 0.5/0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vi1xw74fPUe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRel(balanced='unbalanced', model='DecisionTree')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model = BinaryRel(model = 'DecisionTree')\n",
    "our_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., ..., 1., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 1., 1., ..., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 1., 1., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 1., 0., ..., 1., 0., 1.],\n",
       "        [0., 0., 1., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 1., ..., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 1., ..., 1., 1., 0.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_proba_valid_nosamp=our_model.predict_proba(X_valid)\n",
    "prediction_proba_valid_nosamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show performance of predict_proba, we show with a naibe bayes classsifier, so that we can see probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRel(balanced='unbalanced', model='NaiveBayes')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_modelnb = BinaryRel(model = 'NaiveBayes')\n",
    "our_modelnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[7.30968026e-05, 1.24090631e-01, 9.99529574e-01, ...,\n",
       "         4.83457033e-01, 7.70364660e-01, 1.00000000e+00],\n",
       "        [9.99926903e-01, 8.75909369e-01, 4.70425545e-04, ...,\n",
       "         5.16542967e-01, 2.29635340e-01, 4.57032287e-12]],\n",
       "\n",
       "       [[8.55812873e-01, 3.34898906e-01, 6.27773410e-02, ...,\n",
       "         8.78547885e-01, 8.98386463e-01, 9.77328737e-01],\n",
       "        [1.44187127e-01, 6.65101094e-01, 9.37222659e-01, ...,\n",
       "         1.21452115e-01, 1.01613537e-01, 2.26712631e-02]],\n",
       "\n",
       "       [[9.99847072e-01, 9.99758120e-01, 9.99730909e-01, ...,\n",
       "         1.85365381e-05, 2.82443592e-05, 9.27370310e-01],\n",
       "        [1.52928272e-04, 2.41879581e-04, 2.69090842e-04, ...,\n",
       "         9.99981463e-01, 9.99971756e-01, 7.26296901e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[9.99999862e-01, 9.99999073e-01, 6.53025475e-01, ...,\n",
       "         2.53454299e-04, 1.96871547e-04, 9.19902229e-01],\n",
       "        [1.37647663e-07, 9.26633563e-07, 3.46974525e-01, ...,\n",
       "         9.99746546e-01, 9.99803128e-01, 8.00977715e-02]],\n",
       "\n",
       "       [[3.77959744e-01, 6.59298771e-01, 9.97821482e-01, ...,\n",
       "         2.36093233e-01, 2.13729634e-01, 9.99999995e-01],\n",
       "        [6.22040256e-01, 3.40701229e-01, 2.17851789e-03, ...,\n",
       "         7.63906767e-01, 7.86270366e-01, 4.78829901e-09]],\n",
       "\n",
       "       [[9.98857541e-01, 9.95899781e-01, 9.83359961e-02, ...,\n",
       "         7.04131153e-01, 7.54561223e-01, 9.36325189e-01],\n",
       "        [1.14245917e-03, 4.10021859e-03, 9.01664004e-01, ...,\n",
       "         2.95868847e-01, 2.45438777e-01, 6.36748108e-02]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_proba_valid_nosamp_nb=our_modelnb.predict_proba(X_valid)\n",
    "prediction_proba_valid_nosamp_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4490,
     "status": "error",
     "timestamp": 1582045727420,
     "user": {
      "displayName": "Vitoria Fahed",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCjQgWlu-JChElNuf5JGwyGBtWtmdVOYOx5q6MT=s64",
      "userId": "01295521859694349438"
     },
     "user_tz": 0
    },
    "id": "XnhaxQVQGhuG",
    "outputId": "0537e841-7e0e-44a3-c4a8-0cc221f29f90"
   },
   "outputs": [],
   "source": [
    "# Predicting with validation set\n",
    "prediction_valid_nosamp = our_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to sort classes based on best accuracy for each class for classifier chains\n",
    "accuracy=np.zeros((14,1))\n",
    "for i in range(14):\n",
    "    accuracy[i]=accuracy_score(y_valid.iloc[:,i], prediction_valid_nosamp[:,i])\n",
    "new_column=np.argsort(accuracy,0)  \n",
    "new_column=new_column[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metrics commonly used for a multi-label classification problem are hamming loss and macro-averaged F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.2669716646989374\n",
      "Macro-averaged F1 score: 0.40790487583203255\n"
     ]
    }
   ],
   "source": [
    "# Performance metric calculation (hamming loss and f1_score)\n",
    "hammingloss_valid_nosamp = hamming_loss(y_valid, prediction_valid_nosamp)\n",
    "print(\"Hamming loss: \" + str(hammingloss_valid_nosamp))\n",
    "f1_score_valid_nosamp = metrics.f1_score(y_valid, prediction_valid_nosamp,average='macro')\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_valid_nosamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0371900826446281\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.45      0.45       147\n",
      "           1       0.52      0.51      0.51       207\n",
      "           2       0.57      0.54      0.55       204\n",
      "           3       0.59      0.56      0.58       193\n",
      "           4       0.51      0.49      0.50       146\n",
      "           5       0.35      0.41      0.38       111\n",
      "           6       0.32      0.36      0.34        84\n",
      "           7       0.36      0.38      0.37        97\n",
      "           8       0.09      0.08      0.09        36\n",
      "           9       0.15      0.19      0.17        53\n",
      "          10       0.22      0.18      0.20        61\n",
      "          11       0.80      0.81      0.80       368\n",
      "          12       0.76      0.78      0.77       364\n",
      "          13       0.00      0.00      0.00         6\n",
      "\n",
      "   micro avg       0.56      0.57      0.57      2077\n",
      "   macro avg       0.41      0.41      0.41      2077\n",
      "weighted avg       0.57      0.57      0.57      2077\n",
      " samples avg       0.57      0.57      0.54      2077\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arlene John\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Accuracy calculation (for comparison)\n",
    "accuracy_valid_nosamp = accuracy_score(y_valid, prediction_valid_nosamp)\n",
    "print(\"Accuracy: \" +  str(accuracy_valid_nosamp))\n",
    "print(metrics.classification_report(y_valid, prediction_valid_nosamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.28000787091696183\n",
      "Macro-averaged F1 score: 0.3942115952516226\n",
      "Accuracy: 0.03168044077134986\n"
     ]
    }
   ],
   "source": [
    "# Check performance with pre-built algorithm from scikit-multilearn\n",
    "# Reference: http://scikit.ml/tutorial.html\n",
    "\n",
    "check_alg = BinaryRelevance(classifier = tree.DecisionTreeClassifier(), require_dense=None)\n",
    "tr = check_alg.fit(X_train, y_train)\n",
    "pred = check_alg.predict(X_test)\n",
    "pred_ham = hamming_loss(y_test, pred)\n",
    "pred_f1 = metrics.f1_score(y_test, pred,average='macro')\n",
    "acc_check = accuracy_score(y_test, pred)\n",
    "print(\"Hamming loss: \" + str(pred_ham))\n",
    "print(\"Macro-averaged F1 score: \" + str(pred_f1))\n",
    "print(\"Accuracy: \" + str(acc_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the performance of our algorithm (below) and from a pre-built binary classifier (above) from the skmultilearn library applied to the test set, it is possible to observe that the hamming loss and F1 scores are very similar, almost identical, to that of the classifier designed here. Therefore, our Binary Relevance algorithm can be validated and it is working as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h5ymlozXGh1V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.2730224321133412\n",
      "Macro-averaged F1 score: 0.3981049811365608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[385, 103],\n",
       "        [107, 131]],\n",
       "\n",
       "       [[240, 165],\n",
       "        [152, 169]],\n",
       "\n",
       "       [[296, 147],\n",
       "        [132, 151]],\n",
       "\n",
       "       [[357, 125],\n",
       "        [117, 127]],\n",
       "\n",
       "       [[395, 114],\n",
       "        [116, 101]],\n",
       "\n",
       "       [[411, 139],\n",
       "        [105,  71]],\n",
       "\n",
       "       [[507, 104],\n",
       "        [ 80,  35]],\n",
       "\n",
       "       [[507,  80],\n",
       "        [ 91,  48]],\n",
       "\n",
       "       [[625,  43],\n",
       "        [ 49,   9]],\n",
       "\n",
       "       [[582,  69],\n",
       "        [ 64,  11]],\n",
       "\n",
       "       [[584,  62],\n",
       "        [ 66,  14]],\n",
       "\n",
       "       [[ 57, 137],\n",
       "        [126, 406]],\n",
       "\n",
       "       [[ 53, 146],\n",
       "        [115, 412]],\n",
       "\n",
       "       [[705,  11],\n",
       "        [ 10,   0]]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting with test set and evaluation metrics\n",
    "prediction_test_nosamp = our_model.predict(X_test)\n",
    "hammingloss_test_nosamp = hamming_loss(y_test, prediction_test_nosamp)\n",
    "print(\"Hamming loss: \" + str(hammingloss_test_nosamp))\n",
    "f1_score_test_nosamp = metrics.f1_score(y_test, prediction_test_nosamp,average='macro')\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_test_nosamp))\n",
    "multilabel_confusion_matrix(y_test, prediction_test_nosamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.04269972451790634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.55      0.56       238\n",
      "           1       0.51      0.53      0.52       321\n",
      "           2       0.51      0.53      0.52       283\n",
      "           3       0.50      0.52      0.51       244\n",
      "           4       0.47      0.47      0.47       217\n",
      "           5       0.34      0.40      0.37       176\n",
      "           6       0.25      0.30      0.28       115\n",
      "           7       0.38      0.35      0.36       139\n",
      "           8       0.17      0.16      0.16        58\n",
      "           9       0.14      0.15      0.14        75\n",
      "          10       0.18      0.17      0.18        80\n",
      "          11       0.75      0.76      0.76       532\n",
      "          12       0.74      0.78      0.76       527\n",
      "          13       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.54      0.56      0.55      3015\n",
      "   macro avg       0.39      0.41      0.40      3015\n",
      "weighted avg       0.54      0.56      0.55      3015\n",
      " samples avg       0.55      0.56      0.53      3015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_test_nosamp = accuracy_score(y_test, prediction_test_nosamp)\n",
    "print(\"Accuracy: \" +  str(accuracy_test_nosamp))\n",
    "print(metrics.classification_report(y_test, prediction_test_nosamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store evaluation metrics in the dataframe\n",
    "df['Binary Relavance (BR) without Undersampling']['Hamming Loss ValidSet'] = hammingloss_valid_nosamp\n",
    "df['Binary Relavance (BR) without Undersampling']['F1 Score ValidSet'] = f1_score_valid_nosamp\n",
    "df['Binary Relavance (BR) without Undersampling']['Accuracy ValidSet'] = accuracy_valid_nosamp\n",
    "df['Binary Relavance (BR) without Undersampling']['Hamming Loss TestSet'] = hammingloss_test_nosamp\n",
    "df['Binary Relavance (BR) without Undersampling']['F1 Score TestSet'] = f1_score_test_nosamp\n",
    "df['Binary Relavance (BR) without Undersampling']['Accuracy TestSet'] = accuracy_test_nosamp\n",
    "\n",
    "df2['Binary Relavance without Undersampling']['Hamming Loss ValidSet'] = hammingloss_valid_nosamp\n",
    "df2['Binary Relavance without Undersampling']['F1 Score ValidSet'] = f1_score_valid_nosamp\n",
    "df2['Binary Relavance without Undersampling']['Accuracy ValidSet'] = accuracy_valid_nosamp\n",
    "df2['Binary Relavance without Undersampling']['Hamming Loss TestSet'] = hammingloss_test_nosamp\n",
    "df2['Binary Relavance without Undersampling']['F1 Score TestSet'] = f1_score_test_nosamp\n",
    "df2['Binary Relavance without Undersampling']['Accuracy TestSet'] = accuracy_test_nosamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Binary Relavance (BR) without Undersampling</th>\n",
       "      <th>Binary Relavance with Undersampling</th>\n",
       "      <th>Optimised BR Model with F1 score (w/o undersampling)</th>\n",
       "      <th>Optimised BR Model with Accuracy (w/o undersampling)</th>\n",
       "      <th>Optimised BR Model with F1 score (undersampled)</th>\n",
       "      <th>Optimised BR Model with accuracy (undersampled)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hamming Loss ValidSet</th>\n",
       "      <td>0.266972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score ValidSet</th>\n",
       "      <td>0.407905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy ValidSet</th>\n",
       "      <td>0.037190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming Loss TestSet</th>\n",
       "      <td>0.273022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score TestSet</th>\n",
       "      <td>0.398105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy TestSet</th>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Binary Relavance (BR) without Undersampling  \\\n",
       "Hamming Loss ValidSet                                     0.266972   \n",
       "F1 Score ValidSet                                         0.407905   \n",
       "Accuracy ValidSet                                         0.037190   \n",
       "Hamming Loss TestSet                                      0.273022   \n",
       "F1 Score TestSet                                          0.398105   \n",
       "Accuracy TestSet                                          0.042700   \n",
       "\n",
       "                       Binary Relavance with Undersampling  \\\n",
       "Hamming Loss ValidSet                                  0.0   \n",
       "F1 Score ValidSet                                      0.0   \n",
       "Accuracy ValidSet                                      0.0   \n",
       "Hamming Loss TestSet                                   0.0   \n",
       "F1 Score TestSet                                       0.0   \n",
       "Accuracy TestSet                                       0.0   \n",
       "\n",
       "                       Optimised BR Model with F1 score (w/o undersampling)  \\\n",
       "Hamming Loss ValidSet                                                0.0      \n",
       "F1 Score ValidSet                                                    0.0      \n",
       "Accuracy ValidSet                                                    0.0      \n",
       "Hamming Loss TestSet                                                 0.0      \n",
       "F1 Score TestSet                                                     0.0      \n",
       "Accuracy TestSet                                                     0.0      \n",
       "\n",
       "                       Optimised BR Model with Accuracy (w/o undersampling)  \\\n",
       "Hamming Loss ValidSet                                                0.0      \n",
       "F1 Score ValidSet                                                    0.0      \n",
       "Accuracy ValidSet                                                    0.0      \n",
       "Hamming Loss TestSet                                                 0.0      \n",
       "F1 Score TestSet                                                     0.0      \n",
       "Accuracy TestSet                                                     0.0      \n",
       "\n",
       "                       Optimised BR Model with F1 score (undersampled)  \\\n",
       "Hamming Loss ValidSet                                              0.0   \n",
       "F1 Score ValidSet                                                  0.0   \n",
       "Accuracy ValidSet                                                  0.0   \n",
       "Hamming Loss TestSet                                               0.0   \n",
       "F1 Score TestSet                                                   0.0   \n",
       "Accuracy TestSet                                                   0.0   \n",
       "\n",
       "                       Optimised BR Model with accuracy (undersampled)  \n",
       "Hamming Loss ValidSet                                              0.0  \n",
       "F1 Score ValidSet                                                  0.0  \n",
       "Accuracy ValidSet                                                  0.0  \n",
       "Hamming Loss TestSet                                               0.0  \n",
       "F1 Score TestSet                                                   0.0  \n",
       "Accuracy TestSet                                                   0.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uqogd6tmboOZ"
   },
   "source": [
    "## Task 2: Implement the Binary Relevance Algorithm with Under-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRel(balanced='balanced', model='DecisionTree')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model2 = BinaryRel(model = 'DecisionTree', balanced='balanced')\n",
    "our_model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.423258559622196\n",
      "Macro-averaged F1 score: 0.40745997794431066\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Predicting with validation set and evaluation metrics\n",
    "prediction_valid_samp = our_model2.predict(X_valid)\n",
    "hammingloss_valid_samp = hamming_loss(y_valid, prediction_valid_samp)\n",
    "f1_score_valid_samp = metrics.f1_score(y_valid, prediction_valid_samp,average='macro')\n",
    "accuracy_valid_samp = accuracy_score(y_valid, prediction_valid_samp)\n",
    "print(\"Hamming loss: \" + str(hammingloss_valid_samp))\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_valid_samp))\n",
    "print(\"Accuracy: \" +  str(accuracy_valid_samp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.62      0.50       147\n",
      "           1       0.52      0.56      0.54       207\n",
      "           2       0.58      0.66      0.61       204\n",
      "           3       0.51      0.59      0.55       193\n",
      "           4       0.45      0.66      0.54       146\n",
      "           5       0.31      0.61      0.41       111\n",
      "           6       0.23      0.63      0.34        84\n",
      "           7       0.24      0.56      0.34        97\n",
      "           8       0.09      0.58      0.16        36\n",
      "           9       0.12      0.49      0.19        53\n",
      "          10       0.15      0.57      0.24        61\n",
      "          11       0.78      0.49      0.61       368\n",
      "          12       0.79      0.57      0.66       364\n",
      "          13       0.01      0.50      0.02         6\n",
      "\n",
      "   micro avg       0.38      0.58      0.46      2077\n",
      "   macro avg       0.37      0.58      0.41      2077\n",
      "weighted avg       0.54      0.58      0.53      2077\n",
      " samples avg       0.38      0.58      0.44      2077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_valid, prediction_valid_samp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.42384887839433294\n",
      "Macro-averaged F1 score: 0.39557056936277074\n",
      "Accuracy: 0.0027548209366391185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[316, 172],\n",
       "        [ 85, 153]],\n",
       "\n",
       "       [[221, 184],\n",
       "        [133, 188]],\n",
       "\n",
       "       [[273, 170],\n",
       "        [113, 170]],\n",
       "\n",
       "       [[293, 189],\n",
       "        [ 93, 151]],\n",
       "\n",
       "       [[322, 187],\n",
       "        [ 75, 142]],\n",
       "\n",
       "       [[333, 217],\n",
       "        [ 82,  94]],\n",
       "\n",
       "       [[339, 272],\n",
       "        [ 51,  64]],\n",
       "\n",
       "       [[349, 238],\n",
       "        [ 44,  95]],\n",
       "\n",
       "       [[379, 289],\n",
       "        [ 27,  31]],\n",
       "\n",
       "       [[400, 251],\n",
       "        [ 44,  31]],\n",
       "\n",
       "       [[352, 294],\n",
       "        [ 39,  41]],\n",
       "\n",
       "       [[105,  89],\n",
       "        [276, 256]],\n",
       "\n",
       "       [[110,  89],\n",
       "        [233, 294]],\n",
       "\n",
       "       [[348, 368],\n",
       "        [  4,   6]]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting with test set and evaluation metrics\n",
    "prediction_test_samp = our_model2.predict(X_test)\n",
    "hammingloss_test_samp = hamming_loss(y_test, prediction_test_samp)\n",
    "f1_score_test_samp = metrics.f1_score(y_test, prediction_test_samp,average='macro')\n",
    "accuracy_test_samp = accuracy_score(y_test, prediction_test_samp)\n",
    "print(\"Hamming loss: \" + str(hammingloss_test_samp))\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_test_samp))\n",
    "print(\"Accuracy: \" +  str(accuracy_test_samp))\n",
    "multilabel_confusion_matrix(y_test, prediction_test_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0027548209366391185\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.64      0.54       238\n",
      "           1       0.51      0.59      0.54       321\n",
      "           2       0.50      0.60      0.55       283\n",
      "           3       0.44      0.62      0.52       244\n",
      "           4       0.43      0.65      0.52       217\n",
      "           5       0.30      0.53      0.39       176\n",
      "           6       0.19      0.56      0.28       115\n",
      "           7       0.29      0.68      0.40       139\n",
      "           8       0.10      0.53      0.16        58\n",
      "           9       0.11      0.41      0.17        75\n",
      "          10       0.12      0.51      0.20        80\n",
      "          11       0.74      0.48      0.58       532\n",
      "          12       0.77      0.56      0.65       527\n",
      "          13       0.02      0.60      0.03        10\n",
      "\n",
      "   micro avg       0.36      0.57      0.44      3015\n",
      "   macro avg       0.36      0.57      0.40      3015\n",
      "weighted avg       0.52      0.57      0.51      3015\n",
      " samples avg       0.37      0.58      0.43      3015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" +  str(accuracy_test_samp))\n",
    "print(metrics.classification_report(y_test, prediction_test_samp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store evaluation metrics in dataframe\n",
    "df['Binary Relavance with Undersampling']['Hamming Loss ValidSet'] = hammingloss_valid_samp\n",
    "df['Binary Relavance with Undersampling']['F1 Score ValidSet'] = f1_score_valid_samp\n",
    "df['Binary Relavance with Undersampling']['Accuracy ValidSet'] = accuracy_valid_samp\n",
    "df['Binary Relavance with Undersampling']['Hamming Loss TestSet'] = hammingloss_test_samp\n",
    "df['Binary Relavance with Undersampling']['F1 Score TestSet'] = f1_score_test_samp\n",
    "df['Binary Relavance with Undersampling']['Accuracy TestSet'] = accuracy_test_samp\n",
    "\n",
    "df2['Binary Relavance with Undersampling']['Hamming Loss ValidSet'] = hammingloss_valid_samp\n",
    "df2['Binary Relavance with Undersampling']['F1 Score ValidSet'] = f1_score_valid_samp\n",
    "df2['Binary Relavance with Undersampling']['Accuracy ValidSet'] = accuracy_valid_samp\n",
    "df2['Binary Relavance with Undersampling']['Hamming Loss TestSet'] = hammingloss_test_samp\n",
    "df2['Binary Relavance with Undersampling']['F1 Score TestSet'] = f1_score_test_samp\n",
    "df2['Binary Relavance with Undersampling']['Accuracy TestSet'] = accuracy_test_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Binary Relavance (BR) without Undersampling</th>\n",
       "      <th>Binary Relavance with Undersampling</th>\n",
       "      <th>Optimised BR Model with F1 score (w/o undersampling)</th>\n",
       "      <th>Optimised BR Model with Accuracy (w/o undersampling)</th>\n",
       "      <th>Optimised BR Model with F1 score (undersampled)</th>\n",
       "      <th>Optimised BR Model with accuracy (undersampled)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hamming Loss ValidSet</th>\n",
       "      <td>0.266972</td>\n",
       "      <td>0.423259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score ValidSet</th>\n",
       "      <td>0.407905</td>\n",
       "      <td>0.407460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy ValidSet</th>\n",
       "      <td>0.037190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming Loss TestSet</th>\n",
       "      <td>0.273022</td>\n",
       "      <td>0.423849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score TestSet</th>\n",
       "      <td>0.398105</td>\n",
       "      <td>0.395571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy TestSet</th>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Binary Relavance (BR) without Undersampling  \\\n",
       "Hamming Loss ValidSet                                     0.266972   \n",
       "F1 Score ValidSet                                         0.407905   \n",
       "Accuracy ValidSet                                         0.037190   \n",
       "Hamming Loss TestSet                                      0.273022   \n",
       "F1 Score TestSet                                          0.398105   \n",
       "Accuracy TestSet                                          0.042700   \n",
       "\n",
       "                       Binary Relavance with Undersampling  \\\n",
       "Hamming Loss ValidSet                             0.423259   \n",
       "F1 Score ValidSet                                 0.407460   \n",
       "Accuracy ValidSet                                 0.000000   \n",
       "Hamming Loss TestSet                              0.423849   \n",
       "F1 Score TestSet                                  0.395571   \n",
       "Accuracy TestSet                                  0.002755   \n",
       "\n",
       "                       Optimised BR Model with F1 score (w/o undersampling)  \\\n",
       "Hamming Loss ValidSet                                                0.0      \n",
       "F1 Score ValidSet                                                    0.0      \n",
       "Accuracy ValidSet                                                    0.0      \n",
       "Hamming Loss TestSet                                                 0.0      \n",
       "F1 Score TestSet                                                     0.0      \n",
       "Accuracy TestSet                                                     0.0      \n",
       "\n",
       "                       Optimised BR Model with Accuracy (w/o undersampling)  \\\n",
       "Hamming Loss ValidSet                                                0.0      \n",
       "F1 Score ValidSet                                                    0.0      \n",
       "Accuracy ValidSet                                                    0.0      \n",
       "Hamming Loss TestSet                                                 0.0      \n",
       "F1 Score TestSet                                                     0.0      \n",
       "Accuracy TestSet                                                     0.0      \n",
       "\n",
       "                       Optimised BR Model with F1 score (undersampled)  \\\n",
       "Hamming Loss ValidSet                                              0.0   \n",
       "F1 Score ValidSet                                                  0.0   \n",
       "Accuracy ValidSet                                                  0.0   \n",
       "Hamming Loss TestSet                                               0.0   \n",
       "F1 Score TestSet                                                   0.0   \n",
       "Accuracy TestSet                                                   0.0   \n",
       "\n",
       "                       Optimised BR Model with accuracy (undersampled)  \n",
       "Hamming Loss ValidSet                                              0.0  \n",
       "F1 Score ValidSet                                                  0.0  \n",
       "Accuracy ValidSet                                                  0.0  \n",
       "Hamming Loss TestSet                                               0.0  \n",
       "F1 Score TestSet                                                   0.0  \n",
       "Accuracy TestSet                                                   0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to observe an almost 10 % increase in hamming loss for both the validation set and the test set when undersampling is applied, showing performance drop when undersampling is applied. However, evaluating the model with the F1-score, which is a good measure when we have imbalanced classes, there is no improvement. In reality, the model without under-sampling has a better score when we compare the F1-scores, hamming loss, and accuracy. F1-score does not improve because the F1 score is already a measure that accounts for imbalanced classes. Since we are attempting to correct that with undersampling, no big changes in the F1 score can be observed. Besides, undersampling loses information and that information can be important and useful. With that being said, the undersampling does not improve the performance of the Binary Relevance classifier greatly, however from the confusion matrices, it can be observed that the matrix after the undersampling has more balanced results. </br>\n",
    "\n",
    "It is important to observe, though, that these results are with the Decision Tree classifier as the base classifier. If the base classifier is changed, the metrics may improve with undersampling. Therefore, a good way to evaluate all the possibilities would be to run a Grid Search with all the base classifiers and with and without undersampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "032z4eS1boOh"
   },
   "source": [
    "## Task 3: Compare the Performance of Different Binary Relevance Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best models without undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'model':('DecisionTree','kNN','SVC','NaiveBayes','LogisticRegression', 'Bagging', 'Boosting', 'RandomForest')}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pE0So-8QboOj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=BinaryRel(balanced='unbalanced', model='DecisionTree'),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid=[{'model': ('DecisionTree', 'kNN', 'SVC', 'NaiveBayes',\n",
       "                                    'LogisticRegression', 'Bagging', 'Boosting',\n",
       "                                    'RandomForest')}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_macro', verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid search for best base classifier optimized for f1-score\n",
    "best_model = GridSearchCV(BinaryRel(balanced='unbalanced'),param_grid=param_grid,cv=10,n_jobs=-1, scoring='f1_macro')\n",
    "best_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'model': 'NaiveBayes'}\n",
      "0.4416458137427218\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print(best_model.best_params_)\n",
    "print(best_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.28733766233766234\n",
      "Macro-averaged F1 score: 0.4814404642129134\n",
      "Accuracy: 0.10330578512396695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[265,  72],\n",
       "        [ 58,  89]],\n",
       "\n",
       "       [[155, 122],\n",
       "        [ 61, 146]],\n",
       "\n",
       "       [[196,  84],\n",
       "        [ 52, 152]],\n",
       "\n",
       "       [[237,  54],\n",
       "        [ 81, 112]],\n",
       "\n",
       "       [[272,  66],\n",
       "        [ 68,  78]],\n",
       "\n",
       "       [[267, 106],\n",
       "        [ 44,  67]],\n",
       "\n",
       "       [[277, 123],\n",
       "        [ 40,  44]],\n",
       "\n",
       "       [[273, 114],\n",
       "        [ 41,  56]],\n",
       "\n",
       "       [[342, 106],\n",
       "        [ 20,  16]],\n",
       "\n",
       "       [[326, 105],\n",
       "        [ 28,  25]],\n",
       "\n",
       "       [[328,  95],\n",
       "        [ 35,  26]],\n",
       "\n",
       "       [[ 72,  44],\n",
       "        [128, 240]],\n",
       "\n",
       "       [[ 76,  44],\n",
       "        [128, 236]],\n",
       "\n",
       "       [[452,  26],\n",
       "        [  2,   4]]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on validation set and evaluation metrics\n",
    "prediction_valid_opt1 = best_model.predict(X_valid)\n",
    "hammingloss_valid_opt1 = hamming_loss(y_valid,prediction_valid_opt1)\n",
    "f1_score_valid_opt1 = metrics.f1_score(y_valid,prediction_valid_opt1,average='macro')\n",
    "accuracy_valid_opt1 = accuracy_score(y_valid, prediction_valid_opt1)\n",
    "print(\"Hamming loss: \" + str(hammingloss_valid_opt1))\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_valid_opt1))\n",
    "print(\"Accuracy: \" +  str(accuracy_valid_opt1))\n",
    "multilabel_confusion_matrix(y_valid,prediction_valid_opt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.10330578512396695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.61      0.58       147\n",
      "           1       0.54      0.71      0.61       207\n",
      "           2       0.64      0.75      0.69       204\n",
      "           3       0.67      0.58      0.62       193\n",
      "           4       0.54      0.53      0.54       146\n",
      "           5       0.39      0.60      0.47       111\n",
      "           6       0.26      0.52      0.35        84\n",
      "           7       0.33      0.58      0.42        97\n",
      "           8       0.13      0.44      0.20        36\n",
      "           9       0.19      0.47      0.27        53\n",
      "          10       0.21      0.43      0.29        61\n",
      "          11       0.85      0.65      0.74       368\n",
      "          12       0.84      0.65      0.73       364\n",
      "          13       0.13      0.67      0.22         6\n",
      "\n",
      "   micro avg       0.53      0.62      0.57      2077\n",
      "   macro avg       0.45      0.58      0.48      2077\n",
      "weighted avg       0.62      0.62      0.60      2077\n",
      " samples avg       0.55      0.63      0.56      2077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" +  str(accuracy_valid_opt1))\n",
    "print(metrics.classification_report(y_valid, prediction_valid_opt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.29338842975206614\n",
      "Macro-averaged F1 score: 0.44515024684090815\n",
      "Accuracy: 0.09779614325068871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[392,  96],\n",
       "        [ 80, 158]],\n",
       "\n",
       "       [[231, 174],\n",
       "        [121, 200]],\n",
       "\n",
       "       [[295, 148],\n",
       "        [ 69, 214]],\n",
       "\n",
       "       [[351, 131],\n",
       "        [103, 141]],\n",
       "\n",
       "       [[405, 104],\n",
       "        [100, 117]],\n",
       "\n",
       "       [[397, 153],\n",
       "        [ 81,  95]],\n",
       "\n",
       "       [[450, 161],\n",
       "        [ 51,  64]],\n",
       "\n",
       "       [[426, 161],\n",
       "        [ 72,  67]],\n",
       "\n",
       "       [[535, 133],\n",
       "        [ 43,  15]],\n",
       "\n",
       "       [[519, 132],\n",
       "        [ 42,  33]],\n",
       "\n",
       "       [[495, 151],\n",
       "        [ 54,  26]],\n",
       "\n",
       "       [[ 92, 102],\n",
       "        [181, 351]],\n",
       "\n",
       "       [[ 94, 105],\n",
       "        [180, 347]],\n",
       "\n",
       "       [[671,  45],\n",
       "        [  9,   1]]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on test set and evaluation metrics\n",
    "prediction_test_opt1 = best_model.predict(X_test)\n",
    "hammingloss_test_opt1 = hamming_loss(y_test,prediction_test_opt1)\n",
    "f1_score_test_opt1 = metrics.f1_score(y_test,prediction_test_opt1,average='macro')\n",
    "accuracy_test_opt1 = accuracy_score(y_test, prediction_test_opt1)\n",
    "print(\"Hamming loss: \" + str(hammingloss_test_opt1))\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_test_opt1))\n",
    "print(\"Accuracy: \" +  str(accuracy_test_opt1))\n",
    "multilabel_confusion_matrix(y_test,prediction_test_opt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.09779614325068871\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.66      0.64       238\n",
      "           1       0.53      0.62      0.58       321\n",
      "           2       0.59      0.76      0.66       283\n",
      "           3       0.52      0.58      0.55       244\n",
      "           4       0.53      0.54      0.53       217\n",
      "           5       0.38      0.54      0.45       176\n",
      "           6       0.28      0.56      0.38       115\n",
      "           7       0.29      0.48      0.37       139\n",
      "           8       0.10      0.26      0.15        58\n",
      "           9       0.20      0.44      0.28        75\n",
      "          10       0.15      0.33      0.20        80\n",
      "          11       0.77      0.66      0.71       532\n",
      "          12       0.77      0.66      0.71       527\n",
      "          13       0.02      0.10      0.04        10\n",
      "\n",
      "   micro avg       0.50      0.61      0.55      3015\n",
      "   macro avg       0.41      0.51      0.45      3015\n",
      "weighted avg       0.57      0.61      0.58      3015\n",
      " samples avg       0.52      0.62      0.54      3015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" +  str(accuracy_test_opt1))\n",
    "print(metrics.classification_report(y_test, prediction_test_opt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store evaluation metrics in the dataframe\n",
    "df['Optimised BR Model with F1 score (w/o undersampling)']['Hamming Loss ValidSet'] = hammingloss_valid_opt1\n",
    "df['Optimised BR Model with F1 score (w/o undersampling)']['F1 Score ValidSet'] = f1_score_valid_opt1\n",
    "df['Optimised BR Model with F1 score (w/o undersampling)']['Accuracy ValidSet'] = accuracy_valid_opt1\n",
    "df['Optimised BR Model with F1 score (w/o undersampling)']['Hamming Loss TestSet'] = hammingloss_test_opt1\n",
    "df['Optimised BR Model with F1 score (w/o undersampling)']['F1 Score TestSet'] = f1_score_test_opt1\n",
    "df['Optimised BR Model with F1 score (w/o undersampling)']['Accuracy TestSet'] = accuracy_test_opt1\n",
    "\n",
    "df2['Optimal Binary Relavance without Undersampling (F1 score)']['Hamming Loss ValidSet'] = hammingloss_valid_opt1\n",
    "df2['Optimal Binary Relavance without Undersampling (F1 score)']['F1 Score ValidSet'] = f1_score_valid_opt1\n",
    "df2['Optimal Binary Relavance without Undersampling (F1 score)']['Accuracy ValidSet'] = accuracy_valid_opt1\n",
    "df2['Optimal Binary Relavance without Undersampling (F1 score)']['Hamming Loss TestSet'] = hammingloss_test_opt1\n",
    "df2['Optimal Binary Relavance without Undersampling (F1 score)']['F1 Score TestSet'] = f1_score_test_opt1\n",
    "df2['Optimal Binary Relavance without Undersampling (F1 score)']['Accuracy TestSet'] = accuracy_test_opt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Binary Relavance (BR) without Undersampling</th>\n",
       "      <th>Binary Relavance with Undersampling</th>\n",
       "      <th>Optimised BR Model with F1 score (w/o undersampling)</th>\n",
       "      <th>Optimised BR Model with Accuracy (w/o undersampling)</th>\n",
       "      <th>Optimised BR Model with F1 score (undersampled)</th>\n",
       "      <th>Optimised BR Model with accuracy (undersampled)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hamming Loss ValidSet</th>\n",
       "      <td>0.266972</td>\n",
       "      <td>0.423259</td>\n",
       "      <td>0.287338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score ValidSet</th>\n",
       "      <td>0.407905</td>\n",
       "      <td>0.407460</td>\n",
       "      <td>0.481440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy ValidSet</th>\n",
       "      <td>0.037190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming Loss TestSet</th>\n",
       "      <td>0.273022</td>\n",
       "      <td>0.423849</td>\n",
       "      <td>0.293388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score TestSet</th>\n",
       "      <td>0.398105</td>\n",
       "      <td>0.395571</td>\n",
       "      <td>0.445150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy TestSet</th>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.097796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Binary Relavance (BR) without Undersampling  \\\n",
       "Hamming Loss ValidSet                                     0.266972   \n",
       "F1 Score ValidSet                                         0.407905   \n",
       "Accuracy ValidSet                                         0.037190   \n",
       "Hamming Loss TestSet                                      0.273022   \n",
       "F1 Score TestSet                                          0.398105   \n",
       "Accuracy TestSet                                          0.042700   \n",
       "\n",
       "                       Binary Relavance with Undersampling  \\\n",
       "Hamming Loss ValidSet                             0.423259   \n",
       "F1 Score ValidSet                                 0.407460   \n",
       "Accuracy ValidSet                                 0.000000   \n",
       "Hamming Loss TestSet                              0.423849   \n",
       "F1 Score TestSet                                  0.395571   \n",
       "Accuracy TestSet                                  0.002755   \n",
       "\n",
       "                       Optimised BR Model with F1 score (w/o undersampling)  \\\n",
       "Hamming Loss ValidSet                                           0.287338      \n",
       "F1 Score ValidSet                                               0.481440      \n",
       "Accuracy ValidSet                                               0.103306      \n",
       "Hamming Loss TestSet                                            0.293388      \n",
       "F1 Score TestSet                                                0.445150      \n",
       "Accuracy TestSet                                                0.097796      \n",
       "\n",
       "                       Optimised BR Model with Accuracy (w/o undersampling)  \\\n",
       "Hamming Loss ValidSet                                                0.0      \n",
       "F1 Score ValidSet                                                    0.0      \n",
       "Accuracy ValidSet                                                    0.0      \n",
       "Hamming Loss TestSet                                                 0.0      \n",
       "F1 Score TestSet                                                     0.0      \n",
       "Accuracy TestSet                                                     0.0      \n",
       "\n",
       "                       Optimised BR Model with F1 score (undersampled)  \\\n",
       "Hamming Loss ValidSet                                              0.0   \n",
       "F1 Score ValidSet                                                  0.0   \n",
       "Accuracy ValidSet                                                  0.0   \n",
       "Hamming Loss TestSet                                               0.0   \n",
       "F1 Score TestSet                                                   0.0   \n",
       "Accuracy TestSet                                                   0.0   \n",
       "\n",
       "                       Optimised BR Model with accuracy (undersampled)  \n",
       "Hamming Loss ValidSet                                              0.0  \n",
       "F1 Score ValidSet                                                  0.0  \n",
       "Accuracy ValidSet                                                  0.0  \n",
       "Hamming Loss TestSet                                               0.0  \n",
       "F1 Score TestSet                                                   0.0  \n",
       "Accuracy TestSet                                                   0.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimise the model for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=BinaryRel(balanced='unbalanced', model='DecisionTree'),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid=[{'model': ('DecisionTree', 'kNN', 'SVC', 'NaiveBayes',\n",
       "                                    'LogisticRegression', 'Bagging', 'Boosting',\n",
       "                                    'RandomForest')}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid search for best base classifier optimized for accuracy\n",
    "best_model = GridSearchCV(BinaryRel(balanced='unbalanced'),param_grid=param_grid,cv=10,n_jobs=-1, scoring='accuracy')\n",
    "best_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'model': 'kNN'}\n",
      "0.19553719008264464\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print(best_model.best_params_)\n",
    "print(best_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.2051357733175915\n",
      "Macro-averaged F1 score: 0.439516029675847\n",
      "Accuracy: 0.2066115702479339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[304,  33],\n",
       "        [ 81,  66]],\n",
       "\n",
       "       [[199,  78],\n",
       "        [ 87, 120]],\n",
       "\n",
       "       [[214,  66],\n",
       "        [ 73, 131]],\n",
       "\n",
       "       [[243,  48],\n",
       "        [ 82, 111]],\n",
       "\n",
       "       [[284,  54],\n",
       "        [ 58,  88]],\n",
       "\n",
       "       [[332,  41],\n",
       "        [ 70,  41]],\n",
       "\n",
       "       [[379,  21],\n",
       "        [ 65,  19]],\n",
       "\n",
       "       [[360,  27],\n",
       "        [ 79,  18]],\n",
       "\n",
       "       [[447,   1],\n",
       "        [ 35,   1]],\n",
       "\n",
       "       [[420,  11],\n",
       "        [ 45,   8]],\n",
       "\n",
       "       [[413,  10],\n",
       "        [ 53,   8]],\n",
       "\n",
       "       [[ 18,  98],\n",
       "        [ 34, 334]],\n",
       "\n",
       "       [[ 19, 101],\n",
       "        [ 33, 331]],\n",
       "\n",
       "       [[478,   0],\n",
       "        [  6,   0]]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on validation set and evaluation metrics\n",
    "prediction_valid_opt2 = best_model.predict(X_valid)\n",
    "hammingloss_valid_opt2 = hamming_loss(y_valid,prediction_valid_opt2)\n",
    "f1_score_valid_opt2 = metrics.f1_score(y_valid,prediction_valid_opt2,average='macro')\n",
    "accuracy_valid_opt2 = accuracy_score(y_valid, prediction_valid_opt2)\n",
    "print(\"Hamming loss: \" + str(hammingloss_valid_opt2))\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_valid_opt2))\n",
    "print(\"Accuracy: \" +  str(accuracy_valid_opt2))\n",
    "multilabel_confusion_matrix(y_valid,prediction_valid_opt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2066115702479339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.45      0.54       147\n",
      "           1       0.61      0.58      0.59       207\n",
      "           2       0.66      0.64      0.65       204\n",
      "           3       0.70      0.58      0.63       193\n",
      "           4       0.62      0.60      0.61       146\n",
      "           5       0.50      0.37      0.42       111\n",
      "           6       0.47      0.23      0.31        84\n",
      "           7       0.40      0.19      0.25        97\n",
      "           8       0.50      0.03      0.05        36\n",
      "           9       0.42      0.15      0.22        53\n",
      "          10       0.44      0.13      0.20        61\n",
      "          11       0.77      0.91      0.83       368\n",
      "          12       0.77      0.91      0.83       364\n",
      "          13       0.00      0.00      0.00         6\n",
      "\n",
      "   micro avg       0.68      0.61      0.65      2077\n",
      "   macro avg       0.54      0.41      0.44      2077\n",
      "weighted avg       0.65      0.61      0.62      2077\n",
      " samples avg       0.67      0.61      0.61      2077\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arlene John\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Arlene John\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" +  str(accuracy_valid_opt2))\n",
    "print(metrics.classification_report(y_valid, prediction_valid_opt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.2109405745769382\n",
      "Macro-averaged F1 score: 0.40854376279464155\n",
      "Accuracy: 0.1790633608815427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[439,  49],\n",
       "        [129, 109]],\n",
       "\n",
       "       [[284, 121],\n",
       "        [152, 169]],\n",
       "\n",
       "       [[318, 125],\n",
       "        [106, 177]],\n",
       "\n",
       "       [[390,  92],\n",
       "        [105, 139]],\n",
       "\n",
       "       [[444,  65],\n",
       "        [ 98, 119]],\n",
       "\n",
       "       [[512,  38],\n",
       "        [118,  58]],\n",
       "\n",
       "       [[580,  31],\n",
       "        [ 90,  25]],\n",
       "\n",
       "       [[552,  35],\n",
       "        [113,  26]],\n",
       "\n",
       "       [[666,   2],\n",
       "        [ 57,   1]],\n",
       "\n",
       "       [[638,  13],\n",
       "        [ 70,   5]],\n",
       "\n",
       "       [[634,  12],\n",
       "        [ 76,   4]],\n",
       "\n",
       "       [[ 26, 168],\n",
       "        [ 49, 483]],\n",
       "\n",
       "       [[ 28, 171],\n",
       "        [ 49, 478]],\n",
       "\n",
       "       [[716,   0],\n",
       "        [ 10,   0]]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on test set and evaluation metrics\n",
    "prediction_test_opt2 = best_model.predict(X_test)\n",
    "hammingloss_test_opt2 = hamming_loss(y_test, prediction_test_opt2)\n",
    "f1_score_test_opt2 = metrics.f1_score(y_test, prediction_test_opt2,average='macro')\n",
    "accuracy_test_opt2 = accuracy_score(y_test, prediction_test_opt2)\n",
    "print(\"Hamming loss: \" + str(hammingloss_test_opt2))\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_test_opt2))\n",
    "print(\"Accuracy: \" +  str(accuracy_test_opt2))\n",
    "multilabel_confusion_matrix(y_test,prediction_test_opt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1790633608815427\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.46      0.55       238\n",
      "           1       0.58      0.53      0.55       321\n",
      "           2       0.59      0.63      0.61       283\n",
      "           3       0.60      0.57      0.59       244\n",
      "           4       0.65      0.55      0.59       217\n",
      "           5       0.60      0.33      0.43       176\n",
      "           6       0.45      0.22      0.29       115\n",
      "           7       0.43      0.19      0.26       139\n",
      "           8       0.33      0.02      0.03        58\n",
      "           9       0.28      0.07      0.11        75\n",
      "          10       0.25      0.05      0.08        80\n",
      "          11       0.74      0.91      0.82       532\n",
      "          12       0.74      0.91      0.81       527\n",
      "          13       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.66      0.59      0.63      3015\n",
      "   macro avg       0.49      0.39      0.41      3015\n",
      "weighted avg       0.62      0.59      0.59      3015\n",
      " samples avg       0.65      0.60      0.60      3015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arlene John\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Arlene John\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" +  str(accuracy_test_opt2))\n",
    "print(metrics.classification_report(y_test, prediction_test_opt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store evaluation metrics in the dataframe\n",
    "df['Optimised BR Model with Accuracy (w/o undersampling)']['Hamming Loss ValidSet'] = hammingloss_valid_opt2\n",
    "df['Optimised BR Model with Accuracy (w/o undersampling)']['F1 Score ValidSet'] = f1_score_valid_opt2\n",
    "df['Optimised BR Model with Accuracy (w/o undersampling)']['Accuracy ValidSet'] = accuracy_valid_opt2\n",
    "df['Optimised BR Model with Accuracy (w/o undersampling)']['Hamming Loss TestSet'] = hammingloss_test_opt2\n",
    "df['Optimised BR Model with Accuracy (w/o undersampling)']['F1 Score TestSet'] = f1_score_test_opt2\n",
    "df['Optimised BR Model with Accuracy (w/o undersampling)']['Accuracy TestSet'] = accuracy_test_opt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Binary Relavance (BR) without Undersampling</th>\n",
       "      <th>Binary Relavance with Undersampling</th>\n",
       "      <th>Optimised BR Model with F1 score (w/o undersampling)</th>\n",
       "      <th>Optimised BR Model with Accuracy (w/o undersampling)</th>\n",
       "      <th>Optimised BR Model with F1 score (undersampled)</th>\n",
       "      <th>Optimised BR Model with accuracy (undersampled)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hamming Loss ValidSet</th>\n",
       "      <td>0.266972</td>\n",
       "      <td>0.423259</td>\n",
       "      <td>0.287338</td>\n",
       "      <td>0.205136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score ValidSet</th>\n",
       "      <td>0.407905</td>\n",
       "      <td>0.407460</td>\n",
       "      <td>0.481440</td>\n",
       "      <td>0.439516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy ValidSet</th>\n",
       "      <td>0.037190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103306</td>\n",
       "      <td>0.206612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming Loss TestSet</th>\n",
       "      <td>0.273022</td>\n",
       "      <td>0.423849</td>\n",
       "      <td>0.293388</td>\n",
       "      <td>0.210941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score TestSet</th>\n",
       "      <td>0.398105</td>\n",
       "      <td>0.395571</td>\n",
       "      <td>0.445150</td>\n",
       "      <td>0.408544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy TestSet</th>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.097796</td>\n",
       "      <td>0.179063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Binary Relavance (BR) without Undersampling  \\\n",
       "Hamming Loss ValidSet                                     0.266972   \n",
       "F1 Score ValidSet                                         0.407905   \n",
       "Accuracy ValidSet                                         0.037190   \n",
       "Hamming Loss TestSet                                      0.273022   \n",
       "F1 Score TestSet                                          0.398105   \n",
       "Accuracy TestSet                                          0.042700   \n",
       "\n",
       "                       Binary Relavance with Undersampling  \\\n",
       "Hamming Loss ValidSet                             0.423259   \n",
       "F1 Score ValidSet                                 0.407460   \n",
       "Accuracy ValidSet                                 0.000000   \n",
       "Hamming Loss TestSet                              0.423849   \n",
       "F1 Score TestSet                                  0.395571   \n",
       "Accuracy TestSet                                  0.002755   \n",
       "\n",
       "                       Optimised BR Model with F1 score (w/o undersampling)  \\\n",
       "Hamming Loss ValidSet                                           0.287338      \n",
       "F1 Score ValidSet                                               0.481440      \n",
       "Accuracy ValidSet                                               0.103306      \n",
       "Hamming Loss TestSet                                            0.293388      \n",
       "F1 Score TestSet                                                0.445150      \n",
       "Accuracy TestSet                                                0.097796      \n",
       "\n",
       "                       Optimised BR Model with Accuracy (w/o undersampling)  \\\n",
       "Hamming Loss ValidSet                                           0.205136      \n",
       "F1 Score ValidSet                                               0.439516      \n",
       "Accuracy ValidSet                                               0.206612      \n",
       "Hamming Loss TestSet                                            0.210941      \n",
       "F1 Score TestSet                                                0.408544      \n",
       "Accuracy TestSet                                                0.179063      \n",
       "\n",
       "                       Optimised BR Model with F1 score (undersampled)  \\\n",
       "Hamming Loss ValidSet                                              0.0   \n",
       "F1 Score ValidSet                                                  0.0   \n",
       "Accuracy ValidSet                                                  0.0   \n",
       "Hamming Loss TestSet                                               0.0   \n",
       "F1 Score TestSet                                                   0.0   \n",
       "Accuracy TestSet                                                   0.0   \n",
       "\n",
       "                       Optimised BR Model with accuracy (undersampled)  \n",
       "Hamming Loss ValidSet                                              0.0  \n",
       "F1 Score ValidSet                                                  0.0  \n",
       "Accuracy ValidSet                                                  0.0  \n",
       "Hamming Loss TestSet                                               0.0  \n",
       "F1 Score TestSet                                                   0.0  \n",
       "Accuracy TestSet                                                   0.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters found with Grid Search optimized for the F1 score have indeed improved the F1 score and accuracy for the validation set and the test set. Since the Grid Search was optimized for the F1 score, this is expected. The Hamming Loss increased with the new model. It could because the Naive Bayes classifier is the most appropriate model to optimize the F1 score but might not be optimal for Hamming Loss. \n",
    "\n",
    "The best parameters found with Grid Search optimized for accuracy improved the F1 score and accuracy for the validation set and the test set. Since the Grid Search was optimized for accuracy, this is expected. The Hamming Loss decreased with the new model. This shows that even though the F1 score did not improve as much as when the model was optimized for the F1 score, there has been an overall improvement in performance when optimized for accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best models with undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=BinaryRel(balanced='balanced', model='DecisionTree'),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid=[{'model': ('DecisionTree', 'kNN', 'SVC', 'NaiveBayes',\n",
       "                                    'LogisticRegression', 'Bagging', 'Boosting',\n",
       "                                    'RandomForest')}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_macro', verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid search for best base classifier optimized for f1-score\n",
    "best_model = GridSearchCV(BinaryRel(balanced='balanced'),param_grid=param_grid,cv=10,n_jobs=-1, scoring='f1_macro')\n",
    "best_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'model': 'SVC'}\n",
      "0.4560795077569534\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print(best_model.best_params_)\n",
    "print(best_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.33840023612750886\n",
      "Macro-averaged F1 score: 0.4801737032836975\n",
      "Accuracy: 0.05991735537190083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[259,  78],\n",
       "        [ 46, 101]],\n",
       "\n",
       "       [[173, 104],\n",
       "        [ 57, 150]],\n",
       "\n",
       "       [[206,  74],\n",
       "        [ 52, 152]],\n",
       "\n",
       "       [[221,  70],\n",
       "        [ 42, 151]],\n",
       "\n",
       "       [[281,  57],\n",
       "        [ 50,  96]],\n",
       "\n",
       "       [[251, 122],\n",
       "        [ 42,  69]],\n",
       "\n",
       "       [[297, 103],\n",
       "        [ 40,  44]],\n",
       "\n",
       "       [[219, 168],\n",
       "        [ 35,  62]],\n",
       "\n",
       "       [[232, 216],\n",
       "        [ 10,  26]],\n",
       "\n",
       "       [[266, 165],\n",
       "        [ 16,  37]],\n",
       "\n",
       "       [[244, 179],\n",
       "        [ 24,  37]],\n",
       "\n",
       "       [[ 83,  33],\n",
       "        [153, 215]],\n",
       "\n",
       "       [[ 88,  32],\n",
       "        [155, 209]],\n",
       "\n",
       "       [[312, 166],\n",
       "        [  4,   2]]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on validation set and evaluation metrics\n",
    "prediction_valid_opt3 = best_model.predict(X_valid)\n",
    "hammingloss_valid_opt3 = hamming_loss(y_valid,prediction_valid_opt3)\n",
    "f1_score_valid_opt3 = metrics.f1_score(y_valid,prediction_valid_opt3,average='macro')\n",
    "accuracy_valid_opt3 = accuracy_score(y_valid, prediction_valid_opt3)\n",
    "print(\"Hamming loss: \" + str(hammingloss_valid_opt3))\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_valid_opt3))\n",
    "print(\"Accuracy: \" +  str(accuracy_valid_opt3))\n",
    "multilabel_confusion_matrix(y_valid,prediction_valid_opt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.05991735537190083\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.69      0.62       147\n",
      "           1       0.59      0.72      0.65       207\n",
      "           2       0.67      0.75      0.71       204\n",
      "           3       0.68      0.78      0.73       193\n",
      "           4       0.63      0.66      0.64       146\n",
      "           5       0.36      0.62      0.46       111\n",
      "           6       0.30      0.52      0.38        84\n",
      "           7       0.27      0.64      0.38        97\n",
      "           8       0.11      0.72      0.19        36\n",
      "           9       0.18      0.70      0.29        53\n",
      "          10       0.17      0.61      0.27        61\n",
      "          11       0.87      0.58      0.70       368\n",
      "          12       0.87      0.57      0.69       364\n",
      "          13       0.01      0.33      0.02         6\n",
      "\n",
      "   micro avg       0.46      0.65      0.54      2077\n",
      "   macro avg       0.45      0.64      0.48      2077\n",
      "weighted avg       0.63      0.65      0.61      2077\n",
      " samples avg       0.48      0.65      0.53      2077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" +  str(accuracy_valid_opt3))\n",
    "print(metrics.classification_report(y_valid, prediction_valid_opt3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.3492719401810311\n",
      "Macro-averaged F1 score: 0.45129654161761285\n",
      "Accuracy: 0.07024793388429752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[387, 101],\n",
       "        [ 77, 161]],\n",
       "\n",
       "       [[253, 152],\n",
       "        [114, 207]],\n",
       "\n",
       "       [[308, 135],\n",
       "        [ 80, 203]],\n",
       "\n",
       "       [[330, 152],\n",
       "        [ 62, 182]],\n",
       "\n",
       "       [[417,  92],\n",
       "        [ 80, 137]],\n",
       "\n",
       "       [[362, 188],\n",
       "        [ 60, 116]],\n",
       "\n",
       "       [[451, 160],\n",
       "        [ 57,  58]],\n",
       "\n",
       "       [[367, 220],\n",
       "        [ 56,  83]],\n",
       "\n",
       "       [[371, 297],\n",
       "        [ 27,  31]],\n",
       "\n",
       "       [[410, 241],\n",
       "        [ 32,  43]],\n",
       "\n",
       "       [[403, 243],\n",
       "        [ 36,  44]],\n",
       "\n",
       "       [[117,  77],\n",
       "        [243, 289]],\n",
       "\n",
       "       [[120,  79],\n",
       "        [249, 278]],\n",
       "\n",
       "       [[480, 236],\n",
       "        [  4,   6]]], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on test set and evaluation metrics\n",
    "prediction_test_opt3 = best_model.predict(X_test)\n",
    "hammingloss_test_opt3 = hamming_loss(y_test, prediction_test_opt3)\n",
    "f1_score_test_opt3 = metrics.f1_score(y_test, prediction_test_opt3,average='macro')\n",
    "accuracy_test_opt3 = accuracy_score(y_test, prediction_test_opt3)\n",
    "print(\"Hamming loss: \" + str(hammingloss_test_opt3))\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_test_opt3))\n",
    "print(\"Accuracy: \" +  str(accuracy_test_opt3))\n",
    "multilabel_confusion_matrix(y_test,prediction_test_opt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.07024793388429752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.68      0.64       238\n",
      "           1       0.58      0.64      0.61       321\n",
      "           2       0.60      0.72      0.65       283\n",
      "           3       0.54      0.75      0.63       244\n",
      "           4       0.60      0.63      0.61       217\n",
      "           5       0.38      0.66      0.48       176\n",
      "           6       0.27      0.50      0.35       115\n",
      "           7       0.27      0.60      0.38       139\n",
      "           8       0.09      0.53      0.16        58\n",
      "           9       0.15      0.57      0.24        75\n",
      "          10       0.15      0.55      0.24        80\n",
      "          11       0.79      0.54      0.64       532\n",
      "          12       0.78      0.53      0.63       527\n",
      "          13       0.02      0.60      0.05        10\n",
      "\n",
      "   micro avg       0.44      0.61      0.51      3015\n",
      "   macro avg       0.42      0.61      0.45      3015\n",
      "weighted avg       0.58      0.61      0.57      3015\n",
      " samples avg       0.46      0.63      0.50      3015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" +  str(accuracy_test_opt3))\n",
    "print(metrics.classification_report(y_test, prediction_test_opt3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store evaluation metrics in the dataframe\n",
    "df['Optimised BR Model with F1 score (undersampled)']['Hamming Loss ValidSet'] = hammingloss_valid_opt3\n",
    "df['Optimised BR Model with F1 score (undersampled)']['F1 Score ValidSet'] = f1_score_valid_opt3\n",
    "df['Optimised BR Model with F1 score (undersampled)']['Accuracy ValidSet'] = accuracy_valid_opt3\n",
    "df['Optimised BR Model with F1 score (undersampled)']['Hamming Loss TestSet'] = hammingloss_test_opt3\n",
    "df['Optimised BR Model with F1 score (undersampled)']['F1 Score TestSet'] = f1_score_test_opt3\n",
    "df['Optimised BR Model with F1 score (undersampled)']['Accuracy TestSet'] = accuracy_test_opt3\n",
    "\n",
    "df2['Optimal Binary Relavance with Undersampling (F1 score)']['Hamming Loss ValidSet'] = hammingloss_valid_opt3\n",
    "df2['Optimal Binary Relavance with Undersampling (F1 score)']['F1 Score ValidSet'] = f1_score_valid_opt3\n",
    "df2['Optimal Binary Relavance with Undersampling (F1 score)']['Accuracy ValidSet'] = accuracy_valid_opt3\n",
    "df2['Optimal Binary Relavance with Undersampling (F1 score)']['Hamming Loss TestSet'] = hammingloss_test_opt3\n",
    "df2['Optimal Binary Relavance with Undersampling (F1 score)']['F1 Score TestSet'] = f1_score_test_opt3\n",
    "df2['Optimal Binary Relavance with Undersampling (F1 score)']['Accuracy TestSet'] = accuracy_test_opt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Binary Relavance (BR) without Undersampling</th>\n",
       "      <th>Binary Relavance with Undersampling</th>\n",
       "      <th>Optimised BR Model with F1 score (w/o undersampling)</th>\n",
       "      <th>Optimised BR Model with Accuracy (w/o undersampling)</th>\n",
       "      <th>Optimised BR Model with F1 score (undersampled)</th>\n",
       "      <th>Optimised BR Model with accuracy (undersampled)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hamming Loss ValidSet</th>\n",
       "      <td>0.266972</td>\n",
       "      <td>0.423259</td>\n",
       "      <td>0.287338</td>\n",
       "      <td>0.205136</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score ValidSet</th>\n",
       "      <td>0.407905</td>\n",
       "      <td>0.407460</td>\n",
       "      <td>0.481440</td>\n",
       "      <td>0.439516</td>\n",
       "      <td>0.480174</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy ValidSet</th>\n",
       "      <td>0.037190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103306</td>\n",
       "      <td>0.206612</td>\n",
       "      <td>0.059917</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming Loss TestSet</th>\n",
       "      <td>0.273022</td>\n",
       "      <td>0.423849</td>\n",
       "      <td>0.293388</td>\n",
       "      <td>0.210941</td>\n",
       "      <td>0.349272</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score TestSet</th>\n",
       "      <td>0.398105</td>\n",
       "      <td>0.395571</td>\n",
       "      <td>0.445150</td>\n",
       "      <td>0.408544</td>\n",
       "      <td>0.451297</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy TestSet</th>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.097796</td>\n",
       "      <td>0.179063</td>\n",
       "      <td>0.070248</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Binary Relavance (BR) without Undersampling  \\\n",
       "Hamming Loss ValidSet                                     0.266972   \n",
       "F1 Score ValidSet                                         0.407905   \n",
       "Accuracy ValidSet                                         0.037190   \n",
       "Hamming Loss TestSet                                      0.273022   \n",
       "F1 Score TestSet                                          0.398105   \n",
       "Accuracy TestSet                                          0.042700   \n",
       "\n",
       "                       Binary Relavance with Undersampling  \\\n",
       "Hamming Loss ValidSet                             0.423259   \n",
       "F1 Score ValidSet                                 0.407460   \n",
       "Accuracy ValidSet                                 0.000000   \n",
       "Hamming Loss TestSet                              0.423849   \n",
       "F1 Score TestSet                                  0.395571   \n",
       "Accuracy TestSet                                  0.002755   \n",
       "\n",
       "                       Optimised BR Model with F1 score (w/o undersampling)  \\\n",
       "Hamming Loss ValidSet                                           0.287338      \n",
       "F1 Score ValidSet                                               0.481440      \n",
       "Accuracy ValidSet                                               0.103306      \n",
       "Hamming Loss TestSet                                            0.293388      \n",
       "F1 Score TestSet                                                0.445150      \n",
       "Accuracy TestSet                                                0.097796      \n",
       "\n",
       "                       Optimised BR Model with Accuracy (w/o undersampling)  \\\n",
       "Hamming Loss ValidSet                                           0.205136      \n",
       "F1 Score ValidSet                                               0.439516      \n",
       "Accuracy ValidSet                                               0.206612      \n",
       "Hamming Loss TestSet                                            0.210941      \n",
       "F1 Score TestSet                                                0.408544      \n",
       "Accuracy TestSet                                                0.179063      \n",
       "\n",
       "                       Optimised BR Model with F1 score (undersampled)  \\\n",
       "Hamming Loss ValidSet                                         0.338400   \n",
       "F1 Score ValidSet                                             0.480174   \n",
       "Accuracy ValidSet                                             0.059917   \n",
       "Hamming Loss TestSet                                          0.349272   \n",
       "F1 Score TestSet                                              0.451297   \n",
       "Accuracy TestSet                                              0.070248   \n",
       "\n",
       "                       Optimised BR Model with accuracy (undersampled)  \n",
       "Hamming Loss ValidSet                                              0.0  \n",
       "F1 Score ValidSet                                                  0.0  \n",
       "Accuracy ValidSet                                                  0.0  \n",
       "Hamming Loss TestSet                                               0.0  \n",
       "F1 Score TestSet                                                   0.0  \n",
       "Accuracy TestSet                                                   0.0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize the undersampled model for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=BinaryRel(balanced='balanced', model='DecisionTree'),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid=[{'model': ('DecisionTree', 'kNN', 'SVC', 'NaiveBayes',\n",
       "                                    'LogisticRegression', 'Bagging', 'Boosting',\n",
       "                                    'RandomForest')}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid search for best base classifier optimized for accuracy\n",
    "best_model = GridSearchCV(BinaryRel(balanced='balanced'),param_grid=param_grid,cv=10,n_jobs=-1, scoring='accuracy')\n",
    "best_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'model': 'kNN'}\n",
      "0.04474517906336088\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print(best_model.best_params_)\n",
    "print(best_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.3769185360094451\n",
      "Macro-averaged F1 score: 0.45732759634601133\n",
      "Accuracy: 0.0640495867768595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[241,  96],\n",
       "        [ 53,  94]],\n",
       "\n",
       "       [[168, 109],\n",
       "        [ 69, 138]],\n",
       "\n",
       "       [[192,  88],\n",
       "        [ 54, 150]],\n",
       "\n",
       "       [[214,  77],\n",
       "        [ 56, 137]],\n",
       "\n",
       "       [[219, 119],\n",
       "        [ 43, 103]],\n",
       "\n",
       "       [[221, 152],\n",
       "        [ 37,  74]],\n",
       "\n",
       "       [[238, 162],\n",
       "        [ 22,  62]],\n",
       "\n",
       "       [[210, 177],\n",
       "        [ 34,  63]],\n",
       "\n",
       "       [[236, 212],\n",
       "        [ 13,  23]],\n",
       "\n",
       "       [[251, 180],\n",
       "        [ 20,  33]],\n",
       "\n",
       "       [[249, 174],\n",
       "        [ 24,  37]],\n",
       "\n",
       "       [[ 68,  48],\n",
       "        [159, 209]],\n",
       "\n",
       "       [[ 70,  50],\n",
       "        [136, 228]],\n",
       "\n",
       "       [[290, 188],\n",
       "        [  2,   4]]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on validation set and evaluation metrics\n",
    "prediction_valid_opt4 = best_model.predict(X_valid)\n",
    "hammingloss_valid_opt4 = hamming_loss(y_valid,prediction_valid_opt4)\n",
    "f1_score_valid_opt4 = metrics.f1_score(y_valid,prediction_valid_opt4,average='macro')\n",
    "accuracy_valid_opt4 = accuracy_score(y_valid, prediction_valid_opt4)\n",
    "print(\"Hamming loss: \" + str(hammingloss_valid_opt4))\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_valid_opt4))\n",
    "print(\"Accuracy: \" +  str(accuracy_valid_opt4))\n",
    "multilabel_confusion_matrix(y_valid,prediction_valid_opt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0640495867768595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.64      0.56       147\n",
      "           1       0.56      0.67      0.61       207\n",
      "           2       0.63      0.74      0.68       204\n",
      "           3       0.64      0.71      0.67       193\n",
      "           4       0.46      0.71      0.56       146\n",
      "           5       0.33      0.67      0.44       111\n",
      "           6       0.28      0.74      0.40        84\n",
      "           7       0.26      0.65      0.37        97\n",
      "           8       0.10      0.64      0.17        36\n",
      "           9       0.15      0.62      0.25        53\n",
      "          10       0.18      0.61      0.27        61\n",
      "          11       0.81      0.57      0.67       368\n",
      "          12       0.82      0.63      0.71       364\n",
      "          13       0.02      0.67      0.04         6\n",
      "\n",
      "   micro avg       0.43      0.65      0.51      2077\n",
      "   macro avg       0.41      0.66      0.46      2077\n",
      "weighted avg       0.58      0.65      0.59      2077\n",
      " samples avg       0.45      0.65      0.51      2077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" +  str(accuracy_valid_opt4))\n",
    "print(metrics.classification_report(y_valid, prediction_valid_opt4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.37711530893349077\n",
      "Macro-averaged F1 score: 0.4419340316987174\n",
      "Accuracy: 0.06611570247933884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[331, 157],\n",
       "        [ 87, 151]],\n",
       "\n",
       "       [[250, 155],\n",
       "        [127, 194]],\n",
       "\n",
       "       [[291, 152],\n",
       "        [ 77, 206]],\n",
       "\n",
       "       [[318, 164],\n",
       "        [ 74, 170]],\n",
       "\n",
       "       [[328, 181],\n",
       "        [ 54, 163]],\n",
       "\n",
       "       [[344, 206],\n",
       "        [ 65, 111]],\n",
       "\n",
       "       [[398, 213],\n",
       "        [ 45,  70]],\n",
       "\n",
       "       [[338, 249],\n",
       "        [ 56,  83]],\n",
       "\n",
       "       [[366, 302],\n",
       "        [ 19,  39]],\n",
       "\n",
       "       [[405, 246],\n",
       "        [ 34,  41]],\n",
       "\n",
       "       [[397, 249],\n",
       "        [ 34,  46]],\n",
       "\n",
       "       [[100,  94],\n",
       "        [217, 315]],\n",
       "\n",
       "       [[ 99, 100],\n",
       "        [196, 331]],\n",
       "\n",
       "       [[437, 279],\n",
       "        [  1,   9]]], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on test set and evaluation metrics\n",
    "prediction_test_opt4 = best_model.predict(X_test)\n",
    "hammingloss_test_opt4 = hamming_loss(y_test, prediction_test_opt4)\n",
    "f1_score_test_opt4 = metrics.f1_score(y_test, prediction_test_opt4,average='macro')\n",
    "accuracy_test_opt4 = accuracy_score(y_test, prediction_test_opt4)\n",
    "print(\"Hamming loss: \" + str(hammingloss_test_opt4))\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_test_opt4))\n",
    "print(\"Accuracy: \" +  str(accuracy_test_opt4))\n",
    "multilabel_confusion_matrix(y_test,prediction_test_opt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.06611570247933884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.63      0.55       238\n",
      "           1       0.56      0.60      0.58       321\n",
      "           2       0.58      0.73      0.64       283\n",
      "           3       0.51      0.70      0.59       244\n",
      "           4       0.47      0.75      0.58       217\n",
      "           5       0.35      0.63      0.45       176\n",
      "           6       0.25      0.61      0.35       115\n",
      "           7       0.25      0.60      0.35       139\n",
      "           8       0.11      0.67      0.20        58\n",
      "           9       0.14      0.55      0.23        75\n",
      "          10       0.16      0.57      0.25        80\n",
      "          11       0.77      0.59      0.67       532\n",
      "          12       0.77      0.63      0.69       527\n",
      "          13       0.03      0.90      0.06        10\n",
      "\n",
      "   micro avg       0.41      0.64      0.50      3015\n",
      "   macro avg       0.39      0.65      0.44      3015\n",
      "weighted avg       0.55      0.64      0.57      3015\n",
      " samples avg       0.44      0.65      0.50      3015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" +  str(accuracy_test_opt4))\n",
    "print(metrics.classification_report(y_test, prediction_test_opt4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store evaluation metrics in the dataframe\n",
    "df['Optimised BR Model with accuracy (undersampled)']['Hamming Loss ValidSet'] = hammingloss_valid_opt4\n",
    "df['Optimised BR Model with accuracy (undersampled)']['F1 Score ValidSet'] = f1_score_valid_opt4\n",
    "df['Optimised BR Model with accuracy (undersampled)']['Accuracy ValidSet'] = accuracy_valid_opt4\n",
    "df['Optimised BR Model with accuracy (undersampled)']['Hamming Loss TestSet'] = hammingloss_test_opt4\n",
    "df['Optimised BR Model with accuracy (undersampled)']['F1 Score TestSet'] = f1_score_test_opt4\n",
    "df['Optimised BR Model with accuracy (undersampled)']['Accuracy TestSet'] = accuracy_test_opt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Binary Relavance (BR) without Undersampling</th>\n",
       "      <th>Binary Relavance with Undersampling</th>\n",
       "      <th>Optimised BR Model with F1 score (w/o undersampling)</th>\n",
       "      <th>Optimised BR Model with Accuracy (w/o undersampling)</th>\n",
       "      <th>Optimised BR Model with F1 score (undersampled)</th>\n",
       "      <th>Optimised BR Model with accuracy (undersampled)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hamming Loss ValidSet</th>\n",
       "      <td>0.266972</td>\n",
       "      <td>0.423259</td>\n",
       "      <td>0.287338</td>\n",
       "      <td>0.205136</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.376919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score ValidSet</th>\n",
       "      <td>0.407905</td>\n",
       "      <td>0.407460</td>\n",
       "      <td>0.481440</td>\n",
       "      <td>0.439516</td>\n",
       "      <td>0.480174</td>\n",
       "      <td>0.457328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy ValidSet</th>\n",
       "      <td>0.037190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103306</td>\n",
       "      <td>0.206612</td>\n",
       "      <td>0.059917</td>\n",
       "      <td>0.064050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming Loss TestSet</th>\n",
       "      <td>0.273022</td>\n",
       "      <td>0.423849</td>\n",
       "      <td>0.293388</td>\n",
       "      <td>0.210941</td>\n",
       "      <td>0.349272</td>\n",
       "      <td>0.377115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score TestSet</th>\n",
       "      <td>0.398105</td>\n",
       "      <td>0.395571</td>\n",
       "      <td>0.445150</td>\n",
       "      <td>0.408544</td>\n",
       "      <td>0.451297</td>\n",
       "      <td>0.441934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy TestSet</th>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.097796</td>\n",
       "      <td>0.179063</td>\n",
       "      <td>0.070248</td>\n",
       "      <td>0.066116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Binary Relavance (BR) without Undersampling  \\\n",
       "Hamming Loss ValidSet                                     0.266972   \n",
       "F1 Score ValidSet                                         0.407905   \n",
       "Accuracy ValidSet                                         0.037190   \n",
       "Hamming Loss TestSet                                      0.273022   \n",
       "F1 Score TestSet                                          0.398105   \n",
       "Accuracy TestSet                                          0.042700   \n",
       "\n",
       "                       Binary Relavance with Undersampling  \\\n",
       "Hamming Loss ValidSet                             0.423259   \n",
       "F1 Score ValidSet                                 0.407460   \n",
       "Accuracy ValidSet                                 0.000000   \n",
       "Hamming Loss TestSet                              0.423849   \n",
       "F1 Score TestSet                                  0.395571   \n",
       "Accuracy TestSet                                  0.002755   \n",
       "\n",
       "                       Optimised BR Model with F1 score (w/o undersampling)  \\\n",
       "Hamming Loss ValidSet                                           0.287338      \n",
       "F1 Score ValidSet                                               0.481440      \n",
       "Accuracy ValidSet                                               0.103306      \n",
       "Hamming Loss TestSet                                            0.293388      \n",
       "F1 Score TestSet                                                0.445150      \n",
       "Accuracy TestSet                                                0.097796      \n",
       "\n",
       "                       Optimised BR Model with Accuracy (w/o undersampling)  \\\n",
       "Hamming Loss ValidSet                                           0.205136      \n",
       "F1 Score ValidSet                                               0.439516      \n",
       "Accuracy ValidSet                                               0.206612      \n",
       "Hamming Loss TestSet                                            0.210941      \n",
       "F1 Score TestSet                                                0.408544      \n",
       "Accuracy TestSet                                                0.179063      \n",
       "\n",
       "                       Optimised BR Model with F1 score (undersampled)  \\\n",
       "Hamming Loss ValidSet                                         0.338400   \n",
       "F1 Score ValidSet                                             0.480174   \n",
       "Accuracy ValidSet                                             0.059917   \n",
       "Hamming Loss TestSet                                          0.349272   \n",
       "F1 Score TestSet                                              0.451297   \n",
       "Accuracy TestSet                                              0.070248   \n",
       "\n",
       "                       Optimised BR Model with accuracy (undersampled)  \n",
       "Hamming Loss ValidSet                                         0.376919  \n",
       "F1 Score ValidSet                                             0.457328  \n",
       "Accuracy ValidSet                                             0.064050  \n",
       "Hamming Loss TestSet                                          0.377115  \n",
       "F1 Score TestSet                                              0.441934  \n",
       "Accuracy TestSet                                              0.066116  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters found with Grid Search optimized for the F1 score and undersampling have indeed improved the F1 score and accuracy for the validation set and the test set. Since the Grid Search was optimized for the F1 score, this is expected. The Hamming Loss decreased with the new model. <br/>\n",
    "\n",
    "The best parameters found with Grid Search for accuracy improved the F1 score and accuracy for the validation set and the test set. Since the Grid Search was optimized for accuracy, this is expected. The Hamming Loss decreased with the new model. There has been an overall improvement in performance when optimized with accuracy, however, the improvements are better when the model is optimized for F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Classifiers (Reminder!):\n",
    "\n",
    "Binary Relavance with and without undersampling: Decision Tree\n",
    " <br />\n",
    "Optimised Model with F1 Score for Binary relevance classifiers: SVC with undersampling, Naive Bayes without undersampling\n",
    " <br />\n",
    "Optimised Model with Accuracy: kNN with undersampling, kNN without undersampling\n",
    "\n",
    "Therefore, based on the comparisons made, the Optimal model found by Grid Search optimized for F1-score, Naive Bayes without undersampling and SVC with undersampling are good models to evaluate and test the dataset using Binary Relevance. Accuracy may not be always a reliable measure to evaluate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fBYS9dCwboOm"
   },
   "source": [
    "## Task 4: Implement the Classifier Chains Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tulIgLUeboOo"
   },
   "outputs": [],
   "source": [
    "class ClassifierChainImplement(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \n",
    "    def __init__(self, model = 'DecisionTree'):\n",
    "        self.model = model\n",
    "    \n",
    "   \n",
    "    def fit(self, X_train, Y_train):\n",
    "    #fit function fits a model per label based on the base classifier specified and keeps all the models in a list that is iteratively appended\n",
    "        features = X_train.copy()\n",
    "        labels = Y_train.copy()\n",
    "        if self.model == 'DecisionTree':\n",
    "            our_model = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "        elif self.model == 'kNN':\n",
    "            our_model = neighbors.KNeighborsClassifier()\n",
    "        elif self.model == 'SVC':\n",
    "            our_model = SVC(kernel='rbf')\n",
    "        elif self.model == 'NaiveBayes':\n",
    "            our_model = GaussianNB()\n",
    "        elif self.model == 'LogisticRegression':\n",
    "            our_model = linear_model.LogisticRegression()\n",
    "        elif self.model == 'Bagging':\n",
    "            our_model = ensemble.BaggingClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 50), n_estimators=10)\n",
    "        elif self.model == 'Boosting':\n",
    "            our_model = ensemble.AdaBoostClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 50), n_estimators=10)\n",
    "        elif self.model == 'RandomForest':\n",
    "            our_model = ensemble.RandomForestClassifier(n_estimators = 200, max_features=15, min_samples_split=100)\n",
    "\n",
    "        our_final_model = list()\n",
    "        for i in range(len(labels.columns)):\n",
    "            our_model = our_model.fit(features,labels.iloc[:,i])\n",
    "            result = our_model.predict(features)\n",
    "            heading = 'Pred'+str(i+1) #appending the predicted classes of each model to the features, to be used for training in the next iteration\n",
    "            features[heading] = result\n",
    "            our_final_model.append(copy.deepcopy(our_model))\n",
    "        \n",
    "        self.length_ = len(labels.columns)\n",
    "        self.our_final_model_ = our_final_model\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, features_test):\n",
    "        features_test = X_test.copy()\n",
    "        prediction = np.ones((len(features_test), self.length_))\n",
    "        prediction_proba=np.zeros((len(features_test),2,self.length_))\n",
    "        for k in range(self.length_):\n",
    "            each_model=self.our_final_model_[k]\n",
    "            prediction_proba[:,:,k]=each_model.predict_proba(features_test)\n",
    "            prediction[:,k]=np.argmax(prediction_proba[:,:,k],axis=1)\n",
    "            heading = 'Pred'+str(k+1)\n",
    "            features_test[heading] = prediction[:,k]\n",
    "            \n",
    "            \n",
    "        return prediction_proba\n",
    " \n",
    "    def predict(self, X_test):\n",
    "        #uses the models found during fit and returns predictions as a matrix\n",
    "        features_test = X_test.copy()\n",
    "        prediction = np.ones((len(features_test), self.length_))\n",
    "        for k in range(self.length_):\n",
    "            each_model = self.our_final_model_[k]\n",
    "            prediction[:,k] = each_model.predict(features_test)\n",
    "            heading = 'Pred'+str(k+1)\n",
    "            features_test[heading] = prediction[:,k]\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChainImplement(model='DecisionTree')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_model = ClassifierChainImplement('DecisionTree')\n",
    "cc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., ..., 0., 0., 1.],\n",
       "        [1., 1., 0., ..., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 0., ..., 1., 1., 1.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 0., 1.],\n",
       "        [1., 1., 0., ..., 1., 1., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 0., 1.],\n",
       "        [1., 1., 0., ..., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_probabilities_valid=cc_model.predict_proba(X_valid)\n",
    "predict_probabilities_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.2842384887839433\n",
      "Macro-averaged F1 score: 0.3893301420541208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[254,  83],\n",
       "        [ 78,  69]],\n",
       "\n",
       "       [[163, 114],\n",
       "        [111,  96]],\n",
       "\n",
       "       [[186,  94],\n",
       "        [ 96, 108]],\n",
       "\n",
       "       [[229,  62],\n",
       "        [102,  91]],\n",
       "\n",
       "       [[255,  83],\n",
       "        [ 83,  63]],\n",
       "\n",
       "       [[291,  82],\n",
       "        [ 72,  39]],\n",
       "\n",
       "       [[342,  58],\n",
       "        [ 58,  26]],\n",
       "\n",
       "       [[316,  71],\n",
       "        [ 71,  26]],\n",
       "\n",
       "       [[408,  40],\n",
       "        [ 31,   5]],\n",
       "\n",
       "       [[380,  51],\n",
       "        [ 41,  12]],\n",
       "\n",
       "       [[368,  55],\n",
       "        [ 45,  16]],\n",
       "\n",
       "       [[ 28,  88],\n",
       "        [ 79, 289]],\n",
       "\n",
       "       [[ 32,  88],\n",
       "        [ 77, 287]],\n",
       "\n",
       "       [[471,   7],\n",
       "        [  6,   0]]], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on validation set and evaluation metrics\n",
    "prediction_cc_valid = cc_model.predict(X_valid)\n",
    "hammingloss_cc_valid = hamming_loss(y_valid,prediction_cc_valid)\n",
    "print(\"Hamming loss: \" + str(hammingloss_cc_valid))\n",
    "f1_score_cc_valid = metrics.f1_score(y_valid,prediction_cc_valid,average='macro')\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_cc_valid))\n",
    "multilabel_confusion_matrix(y_valid,prediction_cc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.09917355371900827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.47      0.46       147\n",
      "           1       0.46      0.46      0.46       207\n",
      "           2       0.53      0.53      0.53       204\n",
      "           3       0.59      0.47      0.53       193\n",
      "           4       0.43      0.43      0.43       146\n",
      "           5       0.32      0.35      0.34       111\n",
      "           6       0.31      0.31      0.31        84\n",
      "           7       0.27      0.27      0.27        97\n",
      "           8       0.11      0.14      0.12        36\n",
      "           9       0.19      0.23      0.21        53\n",
      "          10       0.23      0.26      0.24        61\n",
      "          11       0.77      0.79      0.78       368\n",
      "          12       0.77      0.79      0.78       364\n",
      "          13       0.00      0.00      0.00         6\n",
      "\n",
      "   micro avg       0.54      0.54      0.54      2077\n",
      "   macro avg       0.39      0.39      0.39      2077\n",
      "weighted avg       0.54      0.54      0.54      2077\n",
      " samples avg       0.54      0.53      0.51      2077\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arlene John\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "accuracy_cc_valid = accuracy_score(y_valid, prediction_cc_valid)\n",
    "print(\"Accuracy: \" +  str(accuracy_cc_valid))\n",
    "print(metrics.classification_report(y_valid, prediction_cc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.28364817001180637\n",
      "Macro-averaged F1 score: 0.37521964751108944\n",
      "Accuracy: 0.09504132231404959\n"
     ]
    }
   ],
   "source": [
    "# Check performance with pre-built algorithm from scikit-multilearn\n",
    "# Reference: https://xang1234.github.io/multi-label/\n",
    "\n",
    "check_alg2 = ClassifierChain(classifier = tree.DecisionTreeClassifier(), require_dense=None)\n",
    "tr2 = check_alg2.fit(X_train, y_train)\n",
    "pred2 = check_alg2.predict(X_test)\n",
    "pred2_ham = hamming_loss(y_test, pred2)\n",
    "pred2_f1 = metrics.f1_score(y_test, pred2,average='macro')\n",
    "acc2_check = accuracy_score(y_test, pred2)\n",
    "print(\"Hamming loss: \" + str(pred2_ham))\n",
    "print(\"Macro-averaged F1 score: \" + str(pred2_f1))\n",
    "print(\"Accuracy: \" + str(acc2_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the performance of our algorithm (below) and from a pre-built classifier chain (above) from the skmultilearn library, it is possible to observe that the hamming loss and F1 scores are very similar, almost identical, to our classifier chain implementation. Therefore, our Classifier Chain can be validated and it is working as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.282172373081464\n",
      "Macro-averaged F1 score: 0.3773475829343335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[377, 111],\n",
       "        [103, 135]],\n",
       "\n",
       "       [[237, 168],\n",
       "        [162, 159]],\n",
       "\n",
       "       [[311, 132],\n",
       "        [121, 162]],\n",
       "\n",
       "       [[360, 122],\n",
       "        [126, 118]],\n",
       "\n",
       "       [[400, 109],\n",
       "        [119,  98]],\n",
       "\n",
       "       [[436, 114],\n",
       "        [115,  61]],\n",
       "\n",
       "       [[504, 107],\n",
       "        [ 81,  34]],\n",
       "\n",
       "       [[465, 122],\n",
       "        [102,  37]],\n",
       "\n",
       "       [[613,  55],\n",
       "        [ 49,   9]],\n",
       "\n",
       "       [[591,  60],\n",
       "        [ 67,   8]],\n",
       "\n",
       "       [[570,  76],\n",
       "        [ 70,  10]],\n",
       "\n",
       "       [[ 52, 142],\n",
       "        [133, 399]],\n",
       "\n",
       "       [[ 57, 142],\n",
       "        [136, 391]],\n",
       "\n",
       "       [[702,  14],\n",
       "        [ 10,   0]]], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on test set and evaluation metrics\n",
    "prediction_cc_test = cc_model.predict(X_test)\n",
    "hammingloss_cc_test = hamming_loss(y_test,prediction_cc_test)\n",
    "print(\"Hamming loss: \" + str(hammingloss_cc_test))\n",
    "f1_score_cc_test = metrics.f1_score(y_test,prediction_cc_test,average='macro')\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_cc_test))\n",
    "multilabel_confusion_matrix(y_test,prediction_cc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.10606060606060606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.57      0.56       238\n",
      "           1       0.49      0.50      0.49       321\n",
      "           2       0.55      0.57      0.56       283\n",
      "           3       0.49      0.48      0.49       244\n",
      "           4       0.47      0.45      0.46       217\n",
      "           5       0.35      0.35      0.35       176\n",
      "           6       0.24      0.30      0.27       115\n",
      "           7       0.23      0.27      0.25       139\n",
      "           8       0.14      0.16      0.15        58\n",
      "           9       0.12      0.11      0.11        75\n",
      "          10       0.12      0.12      0.12        80\n",
      "          11       0.74      0.75      0.74       532\n",
      "          12       0.73      0.74      0.74       527\n",
      "          13       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.52      0.54      0.53      3015\n",
      "   macro avg       0.37      0.38      0.38      3015\n",
      "weighted avg       0.53      0.54      0.53      3015\n",
      " samples avg       0.53      0.54      0.50      3015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arlene John\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "accuracy_cc_test = accuracy_score(y_test, prediction_cc_test)\n",
    "print(\"Accuracy: \" +  str(accuracy_cc_test))\n",
    "print(metrics.classification_report(y_test, prediction_cc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store evaluation metrics in the dataframe\n",
    "df2['Classifier Chain (CC)']['Hamming Loss ValidSet'] = hammingloss_cc_valid\n",
    "df2['Classifier Chain (CC)']['F1 Score ValidSet'] = f1_score_cc_valid\n",
    "df2['Classifier Chain (CC)']['Accuracy ValidSet'] = accuracy_cc_valid\n",
    "df2['Classifier Chain (CC)']['Hamming Loss TestSet'] = hammingloss_cc_test\n",
    "df2['Classifier Chain (CC)']['F1 Score TestSet'] = f1_score_cc_test\n",
    "df2['Classifier Chain (CC)']['Accuracy TestSet'] = accuracy_cc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Binary Relavance without Undersampling</th>\n",
       "      <th>Binary Relavance with Undersampling</th>\n",
       "      <th>Optimal Binary Relavance without Undersampling (F1 score)</th>\n",
       "      <th>Optimal Binary Relavance with Undersampling (F1 score)</th>\n",
       "      <th>Classifier Chain (CC)</th>\n",
       "      <th>Optimal Classifier Chains (F1 score)</th>\n",
       "      <th>Optimal Changed Chain Sequence CC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hamming Loss ValidSet</th>\n",
       "      <td>0.266972</td>\n",
       "      <td>0.423259</td>\n",
       "      <td>0.287338</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.284238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score ValidSet</th>\n",
       "      <td>0.407905</td>\n",
       "      <td>0.407460</td>\n",
       "      <td>0.481440</td>\n",
       "      <td>0.480174</td>\n",
       "      <td>0.389330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy ValidSet</th>\n",
       "      <td>0.037190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103306</td>\n",
       "      <td>0.059917</td>\n",
       "      <td>0.099174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming Loss TestSet</th>\n",
       "      <td>0.273022</td>\n",
       "      <td>0.423849</td>\n",
       "      <td>0.293388</td>\n",
       "      <td>0.349272</td>\n",
       "      <td>0.282172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score TestSet</th>\n",
       "      <td>0.398105</td>\n",
       "      <td>0.395571</td>\n",
       "      <td>0.445150</td>\n",
       "      <td>0.451297</td>\n",
       "      <td>0.377348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy TestSet</th>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.097796</td>\n",
       "      <td>0.070248</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Binary Relavance without Undersampling  \\\n",
       "Hamming Loss ValidSet                                0.266972   \n",
       "F1 Score ValidSet                                    0.407905   \n",
       "Accuracy ValidSet                                    0.037190   \n",
       "Hamming Loss TestSet                                 0.273022   \n",
       "F1 Score TestSet                                     0.398105   \n",
       "Accuracy TestSet                                     0.042700   \n",
       "\n",
       "                       Binary Relavance with Undersampling  \\\n",
       "Hamming Loss ValidSet                             0.423259   \n",
       "F1 Score ValidSet                                 0.407460   \n",
       "Accuracy ValidSet                                 0.000000   \n",
       "Hamming Loss TestSet                              0.423849   \n",
       "F1 Score TestSet                                  0.395571   \n",
       "Accuracy TestSet                                  0.002755   \n",
       "\n",
       "                       Optimal Binary Relavance without Undersampling (F1 score)  \\\n",
       "Hamming Loss ValidSet                                           0.287338           \n",
       "F1 Score ValidSet                                               0.481440           \n",
       "Accuracy ValidSet                                               0.103306           \n",
       "Hamming Loss TestSet                                            0.293388           \n",
       "F1 Score TestSet                                                0.445150           \n",
       "Accuracy TestSet                                                0.097796           \n",
       "\n",
       "                       Optimal Binary Relavance with Undersampling (F1 score)  \\\n",
       "Hamming Loss ValidSet                                           0.338400        \n",
       "F1 Score ValidSet                                               0.480174        \n",
       "Accuracy ValidSet                                               0.059917        \n",
       "Hamming Loss TestSet                                            0.349272        \n",
       "F1 Score TestSet                                                0.451297        \n",
       "Accuracy TestSet                                                0.070248        \n",
       "\n",
       "                       Classifier Chain (CC)  \\\n",
       "Hamming Loss ValidSet               0.284238   \n",
       "F1 Score ValidSet                   0.389330   \n",
       "Accuracy ValidSet                   0.099174   \n",
       "Hamming Loss TestSet                0.282172   \n",
       "F1 Score TestSet                    0.377348   \n",
       "Accuracy TestSet                    0.106061   \n",
       "\n",
       "                       Optimal Classifier Chains (F1 score)  \\\n",
       "Hamming Loss ValidSet                                   0.0   \n",
       "F1 Score ValidSet                                       0.0   \n",
       "Accuracy ValidSet                                       0.0   \n",
       "Hamming Loss TestSet                                    0.0   \n",
       "F1 Score TestSet                                        0.0   \n",
       "Accuracy TestSet                                        0.0   \n",
       "\n",
       "                       Optimal Changed Chain Sequence CC  \n",
       "Hamming Loss ValidSet                                0.0  \n",
       "F1 Score ValidSet                                    0.0  \n",
       "Accuracy ValidSet                                    0.0  \n",
       "Hamming Loss TestSet                                 0.0  \n",
       "F1 Score TestSet                                     0.0  \n",
       "Accuracy TestSet                                     0.0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9LcluErboOr"
   },
   "source": [
    "## Task 5: Evaluate the Performance of the Classifier Chains Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUOZyQHrboOs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=ClassifierChainImplement(model='DecisionTree'),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid=[{'model': ('DecisionTree', 'kNN', 'SVC', 'NaiveBayes',\n",
       "                                    'LogisticRegression', 'Bagging', 'Boosting',\n",
       "                                    'RandomForest')}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_macro', verbose=0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid search for best base classifier optimized for f1-score\n",
    "param_grid = [{'model':('DecisionTree','kNN','SVC','NaiveBayes','LogisticRegression','Bagging', 'Boosting', 'RandomForest')}]\n",
    "best_model1 = GridSearchCV(ClassifierChainImplement(),param_grid=param_grid,cv=10,n_jobs=-1,scoring='f1_macro')\n",
    "best_model1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'model': 'NaiveBayes'}\n",
      "0.4332779269901894\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print(best_model1.best_params_)\n",
    "print(best_model1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.2989964580873672\n",
      "Macro-averaged F1 score: 0.4671236690962089\n",
      "Accuracy: 0.09297520661157024\n"
     ]
    }
   ],
   "source": [
    "#prediction on validation set and evaluation metrics\n",
    "prediction_cc_valid_opt = best_model1.predict(X_valid)\n",
    "hammingloss_cc_valid_opt = hamming_loss(y_valid,prediction_cc_valid_opt)\n",
    "f1_score_cc_valid_opt = metrics.f1_score(y_valid,prediction_cc_valid_opt,average='macro')\n",
    "accuracy_cc_valid_opt = accuracy_score(y_valid, prediction_cc_valid_opt)\n",
    "print(\"Hamming loss: \" + str(hammingloss_cc_valid_opt))\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_cc_valid_opt))\n",
    "print(\"Accuracy: \" +  str(accuracy_cc_valid_opt))\n",
    "#multilabel_confusion_matrix(y_valid,prediction_cc_valid_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.09297520661157024\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.61      0.58       147\n",
      "           1       0.54      0.71      0.61       207\n",
      "           2       0.65      0.76      0.70       204\n",
      "           3       0.66      0.58      0.61       193\n",
      "           4       0.54      0.54      0.54       146\n",
      "           5       0.39      0.62      0.48       111\n",
      "           6       0.27      0.52      0.35        84\n",
      "           7       0.31      0.54      0.40        97\n",
      "           8       0.12      0.47      0.20        36\n",
      "           9       0.17      0.43      0.25        53\n",
      "          10       0.21      0.48      0.29        61\n",
      "          11       0.87      0.63      0.73       368\n",
      "          12       0.86      0.63      0.73       364\n",
      "          13       0.05      0.50      0.09         6\n",
      "\n",
      "   micro avg       0.51      0.61      0.56      2077\n",
      "   macro avg       0.44      0.57      0.47      2077\n",
      "weighted avg       0.62      0.61      0.60      2077\n",
      " samples avg       0.54      0.62      0.55      2077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" +  str(accuracy_cc_valid_opt))\n",
    "print(metrics.classification_report(y_valid, prediction_cc_valid_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.30794962613144433\n",
      "Macro-averaged F1 score: 0.43883754795097657\n",
      "Accuracy: 0.09228650137741047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[392,  96],\n",
       "        [ 80, 158]],\n",
       "\n",
       "       [[231, 174],\n",
       "        [121, 200]],\n",
       "\n",
       "       [[293, 150],\n",
       "        [ 69, 214]],\n",
       "\n",
       "       [[352, 130],\n",
       "        [104, 140]],\n",
       "\n",
       "       [[400, 109],\n",
       "        [100, 117]],\n",
       "\n",
       "       [[386, 164],\n",
       "        [ 80,  96]],\n",
       "\n",
       "       [[431, 180],\n",
       "        [ 55,  60]],\n",
       "\n",
       "       [[419, 168],\n",
       "        [ 73,  66]],\n",
       "\n",
       "       [[511, 157],\n",
       "        [ 41,  17]],\n",
       "\n",
       "       [[500, 151],\n",
       "        [ 43,  32]],\n",
       "\n",
       "       [[474, 172],\n",
       "        [ 53,  27]],\n",
       "\n",
       "       [[ 99,  95],\n",
       "        [190, 342]],\n",
       "\n",
       "       [[ 99, 100],\n",
       "        [190, 337]],\n",
       "\n",
       "       [[638,  78],\n",
       "        [  7,   3]]], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on test set and evaluation metrics\n",
    "prediction_cc_test_opt = best_model1.predict(X_test)\n",
    "hammingloss_cc_test_opt = hamming_loss(y_test,prediction_cc_test_opt)\n",
    "f1_score_cc_test_opt = metrics.f1_score(y_test,prediction_cc_test_opt,average='macro')\n",
    "accuracy_cc_test_opt = accuracy_score(y_test, prediction_cc_test_opt)\n",
    "print(\"Hamming loss: \" + str(hammingloss_cc_test_opt))\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_cc_test_opt))\n",
    "print(\"Accuracy: \" +  str(accuracy_cc_test_opt))\n",
    "multilabel_confusion_matrix(y_test,prediction_cc_test_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.09228650137741047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.66      0.64       238\n",
      "           1       0.53      0.62      0.58       321\n",
      "           2       0.59      0.76      0.66       283\n",
      "           3       0.52      0.57      0.54       244\n",
      "           4       0.52      0.54      0.53       217\n",
      "           5       0.37      0.55      0.44       176\n",
      "           6       0.25      0.52      0.34       115\n",
      "           7       0.28      0.47      0.35       139\n",
      "           8       0.10      0.29      0.15        58\n",
      "           9       0.17      0.43      0.25        75\n",
      "          10       0.14      0.34      0.19        80\n",
      "          11       0.78      0.64      0.71       532\n",
      "          12       0.77      0.64      0.70       527\n",
      "          13       0.04      0.30      0.07        10\n",
      "\n",
      "   micro avg       0.48      0.60      0.54      3015\n",
      "   macro avg       0.41      0.52      0.44      3015\n",
      "weighted avg       0.57      0.60      0.57      3015\n",
      " samples avg       0.51      0.61      0.53      3015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" +  str(accuracy_cc_test_opt))\n",
    "print(metrics.classification_report(y_test, prediction_cc_test_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store evaluation metrics in the dataframe\n",
    "df2['Optimal Classifier Chains (F1 score)']['Hamming Loss ValidSet'] = hammingloss_cc_valid_opt\n",
    "df2['Optimal Classifier Chains (F1 score)']['F1 Score ValidSet'] = f1_score_cc_valid_opt\n",
    "df2['Optimal Classifier Chains (F1 score)']['Accuracy ValidSet'] = accuracy_cc_valid_opt\n",
    "df2['Optimal Classifier Chains (F1 score)']['Hamming Loss TestSet'] = hammingloss_cc_test_opt\n",
    "df2['Optimal Classifier Chains (F1 score)']['F1 Score TestSet'] = f1_score_cc_test_opt\n",
    "df2['Optimal Classifier Chains (F1 score)']['Accuracy TestSet'] = accuracy_cc_test_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between Binary Relevance and Classifier Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Binary Relavance without Undersampling</th>\n",
       "      <th>Binary Relavance with Undersampling</th>\n",
       "      <th>Optimal Binary Relavance without Undersampling (F1 score)</th>\n",
       "      <th>Optimal Binary Relavance with Undersampling (F1 score)</th>\n",
       "      <th>Classifier Chain (CC)</th>\n",
       "      <th>Optimal Classifier Chains (F1 score)</th>\n",
       "      <th>Optimal Changed Chain Sequence CC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hamming Loss ValidSet</th>\n",
       "      <td>0.266972</td>\n",
       "      <td>0.423259</td>\n",
       "      <td>0.287338</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.284238</td>\n",
       "      <td>0.298996</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score ValidSet</th>\n",
       "      <td>0.407905</td>\n",
       "      <td>0.407460</td>\n",
       "      <td>0.481440</td>\n",
       "      <td>0.480174</td>\n",
       "      <td>0.389330</td>\n",
       "      <td>0.467124</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy ValidSet</th>\n",
       "      <td>0.037190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103306</td>\n",
       "      <td>0.059917</td>\n",
       "      <td>0.099174</td>\n",
       "      <td>0.092975</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming Loss TestSet</th>\n",
       "      <td>0.273022</td>\n",
       "      <td>0.423849</td>\n",
       "      <td>0.293388</td>\n",
       "      <td>0.349272</td>\n",
       "      <td>0.282172</td>\n",
       "      <td>0.307950</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score TestSet</th>\n",
       "      <td>0.398105</td>\n",
       "      <td>0.395571</td>\n",
       "      <td>0.445150</td>\n",
       "      <td>0.451297</td>\n",
       "      <td>0.377348</td>\n",
       "      <td>0.438838</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy TestSet</th>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.097796</td>\n",
       "      <td>0.070248</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.092287</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Binary Relavance without Undersampling  \\\n",
       "Hamming Loss ValidSet                                0.266972   \n",
       "F1 Score ValidSet                                    0.407905   \n",
       "Accuracy ValidSet                                    0.037190   \n",
       "Hamming Loss TestSet                                 0.273022   \n",
       "F1 Score TestSet                                     0.398105   \n",
       "Accuracy TestSet                                     0.042700   \n",
       "\n",
       "                       Binary Relavance with Undersampling  \\\n",
       "Hamming Loss ValidSet                             0.423259   \n",
       "F1 Score ValidSet                                 0.407460   \n",
       "Accuracy ValidSet                                 0.000000   \n",
       "Hamming Loss TestSet                              0.423849   \n",
       "F1 Score TestSet                                  0.395571   \n",
       "Accuracy TestSet                                  0.002755   \n",
       "\n",
       "                       Optimal Binary Relavance without Undersampling (F1 score)  \\\n",
       "Hamming Loss ValidSet                                           0.287338           \n",
       "F1 Score ValidSet                                               0.481440           \n",
       "Accuracy ValidSet                                               0.103306           \n",
       "Hamming Loss TestSet                                            0.293388           \n",
       "F1 Score TestSet                                                0.445150           \n",
       "Accuracy TestSet                                                0.097796           \n",
       "\n",
       "                       Optimal Binary Relavance with Undersampling (F1 score)  \\\n",
       "Hamming Loss ValidSet                                           0.338400        \n",
       "F1 Score ValidSet                                               0.480174        \n",
       "Accuracy ValidSet                                               0.059917        \n",
       "Hamming Loss TestSet                                            0.349272        \n",
       "F1 Score TestSet                                                0.451297        \n",
       "Accuracy TestSet                                                0.070248        \n",
       "\n",
       "                       Classifier Chain (CC)  \\\n",
       "Hamming Loss ValidSet               0.284238   \n",
       "F1 Score ValidSet                   0.389330   \n",
       "Accuracy ValidSet                   0.099174   \n",
       "Hamming Loss TestSet                0.282172   \n",
       "F1 Score TestSet                    0.377348   \n",
       "Accuracy TestSet                    0.106061   \n",
       "\n",
       "                       Optimal Classifier Chains (F1 score)  \\\n",
       "Hamming Loss ValidSet                              0.298996   \n",
       "F1 Score ValidSet                                  0.467124   \n",
       "Accuracy ValidSet                                  0.092975   \n",
       "Hamming Loss TestSet                               0.307950   \n",
       "F1 Score TestSet                                   0.438838   \n",
       "Accuracy TestSet                                   0.092287   \n",
       "\n",
       "                       Optimal Changed Chain Sequence CC  \n",
       "Hamming Loss ValidSet                                0.0  \n",
       "F1 Score ValidSet                                    0.0  \n",
       "Accuracy ValidSet                                    0.0  \n",
       "Hamming Loss TestSet                                 0.0  \n",
       "F1 Score TestSet                                     0.0  \n",
       "Accuracy TestSet                                     0.0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Classifiers:\n",
    "\n",
    "Binary Relavance with and without undersampling: Decision Tree\n",
    " <br />\n",
    "Optimised Model with F1 Score for Binary relevance classifiers: SVC with undersampling, Naive Bayes without undersampling\n",
    " <br />\n",
    "Optimised Model with Accuracy: kNN with undersampling, kNN without undersampling <br />\n",
    "Optimal Classifier Chains: Naive Bayes <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed from the dataframe presented above, the results for the Classifier Chains did not improve the classification process greatly when compared with the binary relevance algorithms. The hamming loss of the the classifier chain is lower than that of the binary relevance classifier. However, the optimal classifier chain which was optimized for F1-score shows an increase in F1-score and accuracy when compared against the normal classifier chain, but performs poorly compared to the binary relevance method in all three metrics considered. The Classifier Chains is theoretically an improvement on the Binary Relevance algorithm because it also considers label dependencies. However, in this case, its performance was not better. It was still a solid performance, with reasonable values for Hamming loss and F1 score. The reasons why the Classifier Chains did not perform could be due to the order of the labels and the chain sequence. If we optimize the chain sequence, the Classifier Chain might exhibit superior performance. Comparing the optimal models of Binary Relevance (SVC with undersampling) and Classifier Chains (Naive Bayes), the Classifier Chains did not outperform the Binary Relevance method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes in Chain Sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate if the chain sequence can be improved to improve its performance, we have ordered the labels based on their scores from the Binary Relevance algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorder the labels based in the decreasing order of accuracy for different labels in binary relevance classifiers (new_column determined in Task 1)\n",
    "new_column=new_column.T[0]\n",
    "y_train_new=y_train.iloc[:,new_column]\n",
    "y_valid_new=y_valid.iloc[:,new_column]\n",
    "y_test_new=y_test.iloc[:,new_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=ClassifierChainImplement(model='DecisionTree'),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid=[{'model': ('DecisionTree', 'kNN', 'SVC', 'NaiveBayes',\n",
       "                                    'LogisticRegression', 'Bagging', 'Boosting',\n",
       "                                    'RandomForest')}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_macro', verbose=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid search for best base classifier optimized for f1-score\n",
    "param_grid = [{'model':('DecisionTree','kNN','SVC','NaiveBayes','LogisticRegression','Bagging', 'Boosting', 'RandomForest')}]\n",
    "best_model_cc_cs = GridSearchCV(ClassifierChainImplement(),param_grid=param_grid,cv=10,n_jobs=-1,scoring='f1_macro')\n",
    "best_model_cc_cs.fit(X_train,y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'model': 'NaiveBayes'}\n",
      "0.4404358941407335\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print(best_model_cc_cs.best_params_)\n",
    "print(best_model_cc_cs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Sequence Chain Optimised\n",
      " \n",
      " \n",
      "Validation Set:\n",
      " \n",
      "Hamming loss: 0.2953069657615112\n",
      "Macro-averaged F1 score: 0.4784839761487472\n",
      "Accuracy: 0.10743801652892562\n",
      " \n",
      "Test Set:\n",
      " \n",
      "Hamming loss: 0.30509641873278237\n",
      "Macro-averaged F1 score: 0.43778615836460055\n",
      "Accuracy: 0.10055096418732783\n"
     ]
    }
   ],
   "source": [
    "#prediction on validation set and test set and evaluation metrics\n",
    "print(' ')\n",
    "print('Sequence Chain Optimised')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('Validation Set:')\n",
    "print(' ')\n",
    "prediction_cc_valid_cs = best_model_cc_cs.predict(X_valid)\n",
    "hammingloss_cc_valid_cs = hamming_loss(y_valid_new,prediction_cc_valid_cs)\n",
    "print(\"Hamming loss: \" + str(hammingloss_cc_valid_cs))\n",
    "f1_score_cc_valid_cs = metrics.f1_score(y_valid_new,prediction_cc_valid_cs,average='macro')\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_cc_valid_cs))\n",
    "multilabel_confusion_matrix(y_valid_new,prediction_cc_valid_cs)\n",
    "accuracy_cc_valid_cs = accuracy_score(y_valid_new, prediction_cc_valid_cs)\n",
    "print(\"Accuracy: \" +  str(accuracy_cc_valid_cs))\n",
    "print(' ')\n",
    "print('Test Set:')\n",
    "print(' ')\n",
    "prediction_cc_test_cs = best_model_cc_cs.predict(X_test)\n",
    "hammingloss_cc_test_cs = hamming_loss(y_test_new, prediction_cc_test_cs)\n",
    "print(\"Hamming loss: \" + str(hammingloss_cc_test_cs))\n",
    "f1_score_cc_test_cs = metrics.f1_score(y_test_new, prediction_cc_test_cs,average='macro')\n",
    "print(\"Macro-averaged F1 score: \" + str(f1_score_cc_test_cs))\n",
    "multilabel_confusion_matrix(y_test_new,prediction_cc_test_cs)\n",
    "accuracy_cc_test_cs = accuracy_score(y_test_new, prediction_cc_test_cs)\n",
    "print(\"Accuracy: \" +  str(accuracy_cc_test_cs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store evaluation metrics in the dataframe\n",
    "df2['Optimal Changed Chain Sequence CC']['Hamming Loss ValidSet'] = hammingloss_cc_valid_cs\n",
    "df2['Optimal Changed Chain Sequence CC']['F1 Score ValidSet'] = f1_score_cc_valid_cs\n",
    "df2['Optimal Changed Chain Sequence CC']['Accuracy ValidSet'] = accuracy_cc_valid_cs\n",
    "\n",
    "df2['Optimal Changed Chain Sequence CC']['Hamming Loss TestSet'] = hammingloss_cc_test_cs\n",
    "df2['Optimal Changed Chain Sequence CC']['F1 Score TestSet'] = f1_score_cc_test_cs\n",
    "df2['Optimal Changed Chain Sequence CC']['Accuracy TestSet'] = accuracy_cc_test_cs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Binary Relavance without Undersampling</th>\n",
       "      <th>Binary Relavance with Undersampling</th>\n",
       "      <th>Optimal Binary Relavance without Undersampling (F1 score)</th>\n",
       "      <th>Optimal Binary Relavance with Undersampling (F1 score)</th>\n",
       "      <th>Classifier Chain (CC)</th>\n",
       "      <th>Optimal Classifier Chains (F1 score)</th>\n",
       "      <th>Optimal Changed Chain Sequence CC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hamming Loss ValidSet</th>\n",
       "      <td>0.266972</td>\n",
       "      <td>0.423259</td>\n",
       "      <td>0.287338</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.284238</td>\n",
       "      <td>0.298996</td>\n",
       "      <td>0.295307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score ValidSet</th>\n",
       "      <td>0.407905</td>\n",
       "      <td>0.407460</td>\n",
       "      <td>0.481440</td>\n",
       "      <td>0.480174</td>\n",
       "      <td>0.389330</td>\n",
       "      <td>0.467124</td>\n",
       "      <td>0.478484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy ValidSet</th>\n",
       "      <td>0.037190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103306</td>\n",
       "      <td>0.059917</td>\n",
       "      <td>0.099174</td>\n",
       "      <td>0.092975</td>\n",
       "      <td>0.107438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming Loss TestSet</th>\n",
       "      <td>0.273022</td>\n",
       "      <td>0.423849</td>\n",
       "      <td>0.293388</td>\n",
       "      <td>0.349272</td>\n",
       "      <td>0.282172</td>\n",
       "      <td>0.307950</td>\n",
       "      <td>0.305096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score TestSet</th>\n",
       "      <td>0.398105</td>\n",
       "      <td>0.395571</td>\n",
       "      <td>0.445150</td>\n",
       "      <td>0.451297</td>\n",
       "      <td>0.377348</td>\n",
       "      <td>0.438838</td>\n",
       "      <td>0.437786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy TestSet</th>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.097796</td>\n",
       "      <td>0.070248</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.092287</td>\n",
       "      <td>0.100551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Binary Relavance without Undersampling  \\\n",
       "Hamming Loss ValidSet                                0.266972   \n",
       "F1 Score ValidSet                                    0.407905   \n",
       "Accuracy ValidSet                                    0.037190   \n",
       "Hamming Loss TestSet                                 0.273022   \n",
       "F1 Score TestSet                                     0.398105   \n",
       "Accuracy TestSet                                     0.042700   \n",
       "\n",
       "                       Binary Relavance with Undersampling  \\\n",
       "Hamming Loss ValidSet                             0.423259   \n",
       "F1 Score ValidSet                                 0.407460   \n",
       "Accuracy ValidSet                                 0.000000   \n",
       "Hamming Loss TestSet                              0.423849   \n",
       "F1 Score TestSet                                  0.395571   \n",
       "Accuracy TestSet                                  0.002755   \n",
       "\n",
       "                       Optimal Binary Relavance without Undersampling (F1 score)  \\\n",
       "Hamming Loss ValidSet                                           0.287338           \n",
       "F1 Score ValidSet                                               0.481440           \n",
       "Accuracy ValidSet                                               0.103306           \n",
       "Hamming Loss TestSet                                            0.293388           \n",
       "F1 Score TestSet                                                0.445150           \n",
       "Accuracy TestSet                                                0.097796           \n",
       "\n",
       "                       Optimal Binary Relavance with Undersampling (F1 score)  \\\n",
       "Hamming Loss ValidSet                                           0.338400        \n",
       "F1 Score ValidSet                                               0.480174        \n",
       "Accuracy ValidSet                                               0.059917        \n",
       "Hamming Loss TestSet                                            0.349272        \n",
       "F1 Score TestSet                                                0.451297        \n",
       "Accuracy TestSet                                                0.070248        \n",
       "\n",
       "                       Classifier Chain (CC)  \\\n",
       "Hamming Loss ValidSet               0.284238   \n",
       "F1 Score ValidSet                   0.389330   \n",
       "Accuracy ValidSet                   0.099174   \n",
       "Hamming Loss TestSet                0.282172   \n",
       "F1 Score TestSet                    0.377348   \n",
       "Accuracy TestSet                    0.106061   \n",
       "\n",
       "                       Optimal Classifier Chains (F1 score)  \\\n",
       "Hamming Loss ValidSet                              0.298996   \n",
       "F1 Score ValidSet                                  0.467124   \n",
       "Accuracy ValidSet                                  0.092975   \n",
       "Hamming Loss TestSet                               0.307950   \n",
       "F1 Score TestSet                                   0.438838   \n",
       "Accuracy TestSet                                   0.092287   \n",
       "\n",
       "                       Optimal Changed Chain Sequence CC  \n",
       "Hamming Loss ValidSet                           0.295307  \n",
       "F1 Score ValidSet                               0.478484  \n",
       "Accuracy ValidSet                               0.107438  \n",
       "Hamming Loss TestSet                            0.305096  \n",
       "F1 Score TestSet                                0.437786  \n",
       "Accuracy TestSet                                0.100551  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Classifiers:\n",
    "\n",
    "Binary Relavance with and without undersampling: Decision Tree\n",
    " <br />\n",
    "Optimised Model with F1 Score for Binary relevance classifiers: SVC with undersampling, Naive Bayes without undersampling\n",
    " <br />\n",
    "Optimised Model with Accuracy: kNN with undersampling, kNN without undersampling <br />\n",
    "Optimal Classifier Chains: Naive Bayes <br/>\n",
    "Optimal Chain Sequenced CC: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the chaining sequence and finding the optimal base classifier shows a slightly better performance than the simple optimized chain classifier in terms of accuracy, F1 score and hamming loss in the validation set. However, in the test set, the original classifier chain has a better F1 score. <br/>\n",
    "However, when compared against the best performing binary relevance classifier, the optimized changed chain sequence-based classifier chain still performs poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BtxKCUEXboOw"
   },
   "source": [
    "## Task 6: Reflect on the Performance of the Different Models Evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "908Svq0dboOx"
   },
   "source": [
    "The models approached in this assignment were the Binary Relevance (BR) and the Classifier Chains (CC). BR treats the multilabel classification by trying to learn a different model for each label. In this assignment, we also applied undersampling to try to solve the problem of imbalanced classes. CC is a more complex learner than the Binary Relevance classifier as it considers label dependencies and includes the prediction of labels as input for new predictions. CCs get computationally more expensive for each subsequent label classifier as the number of feature increase. Theoretically, it is expected that the CC performs better than the BR for the fact it considers label dependencies. However, both algorithms exhibited similar performance, with the Binary relevance classifier being slightly better. This could be attributed to the non-optimal chain sequence when building the CC. To investigate this, we tried a simple binary relevance performance-based chain sequence optimization method. It showed improvements when compared against a normal classifier chain but still did not outperform the Binary Relevance classifier for this dataset. Undersampling the dataset was helpful to address the imbalanced classes. However, the undersampling showed a drop in performance for the metrics considered here. This could be due to the loss of information due to undersampling and the subsequent loss in significant training examples. Thus, the Binary Relevance method proved to be an effective method in tackling this problem, since it is comparatively less complex and computationally less expensive. It is worth noting that neither of the algorithms exhibited outstanding results and optimization methods could be applied to try to improve them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-QWSVZ3YboOy"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment1_MultiLabelClassification_Template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
